{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In context learning and Prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: c:\\Users\\daria\\OneDrive\\Escritorio\\UCL\\Dissertation\\Negations-LM\\context\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The man drank from the tap. <|perturb|> [negation] The man [BLANK] drank from the tap.', 'The man drank from the tap. <|perturb|> [negation] The man [BLANK] from the tap.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "INFO:polyjuice.polyjuice_wrapper:Setup SpaCy processor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE\n",
      "[[{'generated_text': \"The man drank from the tap. <|perturb|> [negation] The man [BLANK] drank from the tap. [SEP] didn't [ANSWER] \"}, {'generated_text': 'The man drank from the tap. <|perturb|> [negation] The man [BLANK] drank from the tap. [SEP] never [ANSWER] '}, {'generated_text': 'The man drank from the tap. <|perturb|> [negation] The man [BLANK] drank from the tap. [SEP] did not [ANSWER] '}], [{'generated_text': \"The man drank from the tap. <|perturb|> [negation] The man [BLANK] from the tap. [SEP] didn't drink [ANSWER] \"}, {'generated_text': 'The man drank from the tap. <|perturb|> [negation] The man [BLANK] from the tap. [SEP] did not drink [ANSWER] '}, {'generated_text': 'The man drank from the tap. <|perturb|> [negation] The man [BLANK] from the tap. [SEP] drank no more [ANSWER] '}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:polyjuice.polyjuice_wrapper:Setup perplexity scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The man never drank from the tap.', 'The man did not drink from the tap.', \"The man didn't drink from the tap.\", 'The man drank no more from the tap.']\n"
     ]
    }
   ],
   "source": [
    "# NEW VERSION\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#project_root = r'C:\\Users\\daria\\OneDrive\\Escritorio\\UCL\\Dissertation\\Negations-LM\\notebook'\n",
    "\n",
    "#C:\\Users\\daria\\OneDrive\\Escritorio\\UCL\\Dissertation\\Negations-LM\\context\\incontext.py\n",
    "#os.chdir(project_root)\n",
    "\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "from incontext import PerturbationGenerator\n",
    "\n",
    "# Example usage\n",
    "text = \"The project is sustainable.\"\n",
    "random_blanks = {'The project [BLANK] sustainable', '[BLANK] project is sustainable'}\n",
    "\n",
    "text = \"The argument is complicated.\"\n",
    "random_blanks = {'The argument [BLANK] complicated.', 'The argument is [BLANK].'}\n",
    "\n",
    "text = \"The man drank from the tap.\"\n",
    "random_blanks = {'The man [BLANK] from the tap.', 'The man [BLANK] drank from the tap.'}\n",
    "\n",
    "generator = PerturbationGenerator()\n",
    "\n",
    "prompts = generator.get_prompts(text, random_blanks)\n",
    "print(prompts)\n",
    "\n",
    "generated_sequences = generator.generate_on_prompts(prompts, temperature=1, num_beams=5, do_sample=True, batch_size=128, num_return_sequences=3)\n",
    "\n",
    "listado = generator.validate_and_sample_perturbations(generated_sequences, text, perplex_thred=10, num_perturbations=4)\n",
    "print(listado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('negation', \"The man didn't drank from the tap.\"),\n",
       "  ('negation', 'The man did not drank from the tap.'),\n",
       "  ('negation', 'The man never drank from the tap.')],\n",
       " [('negation', 'The man drank no more from the tap.'),\n",
       "  ('negation', \"The man didn't drink from the tap.\"),\n",
       "  ('negation', 'The man did not drink from the tap.')]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It is possible. <|perturb|> [negation] It is [BLANK]. [SEP] impossible [ANSWER]\\nHe is satisfied. <|perturb|> [negation] He is [BLANK]. [SEP] dissatisfied [ANSWER]\\nThe statement is logical. <|perturb|> [negation] The statement is [BLANK]. [SEP] illogical [ANSWER]\\nShe is happy. <|perturb|> [negation] She is [BLANK]. [SEP] unhappy [ANSWER]\\nThe idea is common. <|perturb|> [negation] The idea is [BLANK]. [SEP] uncommon [ANSWER]\\nHis behavior is always responsible. <|perturb|> [negation] His behavior is always [BLANK].', 'It is possible. <|perturb|> [negation] It is [BLANK]. [SEP] impossible [ANSWER]\\nHe is satisfied. <|perturb|> [negation] He is [BLANK]. [SEP] dissatisfied [ANSWER]\\nThe statement is logical. <|perturb|> [negation] The statement is [BLANK]. [SEP] illogical [ANSWER]\\nShe is happy. <|perturb|> [negation] She is [BLANK]. [SEP] unhappy [ANSWER]\\nThe idea is common. <|perturb|> [negation] The idea is [BLANK]. [SEP] uncommon [ANSWER]\\nHis behavior is always responsible. <|perturb|> [negation] His behavior is [BLANK] responsible.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "INFO:polyjuice.polyjuice_wrapper:Setup SpaCy processor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE\n",
      "[[{'generated_text': 'It is possible. <|perturb|> [negation] It is [BLANK]. [SEP] impossible [ANSWER]\\nHe is satisfied. <|perturb|> [negation] He is [BLANK]. [SEP] dissatisfied [ANSWER]\\nThe statement is logical. <|perturb|> [negation] The statement is [BLANK]. [SEP] illogical [ANSWER]\\nShe is happy. <|perturb|> [negation] She is [BLANK]. [SEP] unhappy [ANSWER]\\nThe idea is common. <|perturb|> [negation] The idea is [BLANK]. [SEP] uncommon [ANSWER]\\nHis behavior is always responsible. <|perturb|> [negation] His behavior is always [BLANK]. [ANSWER] '}, {'generated_text': 'It is possible. <|perturb|> [negation] It is [BLANK]. [SEP] impossible [ANSWER]\\nHe is satisfied. <|perturb|> [negation] He is [BLANK]. [SEP] dissatisfied [ANSWER]\\nThe statement is logical. <|perturb|> [negation] The statement is [BLANK]. [SEP] illogical [ANSWER]\\nShe is happy. <|perturb|> [negation] She is [BLANK]. [SEP] unhappy [ANSWER]\\nThe idea is common. <|perturb|> [negation] The idea is [BLANK]. [SEP] uncommon [ANSWER]\\nHis behavior is always responsible. <|perturb|> [negation] His behavior is always [BLANK]. [SEP] not responsible. [ANSWER] '}, {'generated_text': 'It is possible. <|perturb|> [negation] It is [BLANK]. [SEP] impossible [ANSWER]\\nHe is satisfied. <|perturb|> [negation] He is [BLANK]. [SEP] dissatisfied [ANSWER]\\nThe statement is logical. <|perturb|> [negation] The statement is [BLANK]. [SEP] illogical [ANSWER]\\nShe is happy. <|perturb|> [negation] She is [BLANK]. [SEP] unhappy [ANSWER]\\nThe idea is common. <|perturb|> [negation] The idea is [BLANK]. [SEP] uncommon [ANSWER]\\nHis behavior is always responsible. <|perturb|> [negation] His behavior is always [BLANK]. [SEP] not [ANSWER] '}], [{'generated_text': 'It is possible. <|perturb|> [negation] It is [BLANK]. [SEP] impossible [ANSWER]\\nHe is satisfied. <|perturb|> [negation] He is [BLANK]. [SEP] dissatisfied [ANSWER]\\nThe statement is logical. <|perturb|> [negation] The statement is [BLANK]. [SEP] illogical [ANSWER]\\nShe is happy. <|perturb|> [negation] She is [BLANK]. [SEP] unhappy [ANSWER]\\nThe idea is common. <|perturb|> [negation] The idea is [BLANK]. [SEP] uncommon [ANSWER]\\nHis behavior is always responsible. <|perturb|> [negation] His behavior is [BLANK] responsible. [ANSWER] '}, {'generated_text': 'It is possible. <|perturb|> [negation] It is [BLANK]. [SEP] impossible [ANSWER]\\nHe is satisfied. <|perturb|> [negation] He is [BLANK]. [SEP] dissatisfied [ANSWER]\\nThe statement is logical. <|perturb|> [negation] The statement is [BLANK]. [SEP] illogical [ANSWER]\\nShe is happy. <|perturb|> [negation] She is [BLANK]. [SEP] unhappy [ANSWER]\\nThe idea is common. <|perturb|> [negation] The idea is [BLANK]. [SEP] uncommon [ANSWER]\\nHis behavior is always responsible. <|perturb|> [negation] His behavior is [BLANK] responsible. ) [ANSWER] '}, {'generated_text': 'It is possible. <|perturb|> [negation] It is [BLANK]. [SEP] impossible [ANSWER]\\nHe is satisfied. <|perturb|> [negation] He is [BLANK]. [SEP] dissatisfied [ANSWER]\\nThe statement is logical. <|perturb|> [negation] The statement is [BLANK]. [SEP] illogical [ANSWER]\\nShe is happy. <|perturb|> [negation] She is [BLANK]. [SEP] unhappy [ANSWER]\\nThe idea is common. <|perturb|> [negation] The idea is [BLANK]. [SEP] uncommon [ANSWER]\\nHis behavior is always responsible. <|perturb|> [negation] His behavior is [BLANK] responsible. \" [ANSWER] '}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:polyjuice.polyjuice_wrapper:Setup perplexity scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His behavior is always not responsible..', 'His behavior is always not.']\n"
     ]
    }
   ],
   "source": [
    "#text = \"The project is sustainable.\"\n",
    "text = \"The argument is credible.\"\n",
    "random_blanks = {'The argument [BLANK] credible.', 'The argument is [BLANK].'}\n",
    "\n",
    "examples = [\n",
    "    \"It is great for kids. <|perturb|> [negation] It [BLANK] great for [BLANK]. [SEP] is not [ANSWER]\",\n",
    "    \"The weather is sunny. <|perturb|> [negation] The weather [BLANK] sunny. [SEP] is not [ANSWER]\",\n",
    "    \"He enjoys reading books. <|perturb|> [negation] He [BLANK] enjoys reading books. [SEP] does not [ANSWER]\",\n",
    "    #\"It is a sunny day. <|perturb|> [negation] It [BLANK] a sunny day. [SEP] is not [ANSWER]\"\n",
    "]\n",
    "\n",
    "examples_affixal = [\n",
    "\"It is possible. <|perturb|> [negation] It is [BLANK]. [SEP] impossible [ANSWER]\",\n",
    "\"He is satisfied. <|perturb|> [negation] He is [BLANK]. [SEP] dissatisfied [ANSWER]\",\n",
    "\"The statement is logical. <|perturb|> [negation] The statement is [BLANK]. [SEP] illogical [ANSWER]\",\n",
    "#\"The instructions were straightforward and uncomplicated. <|perturb|> [negation] The instructions were straightforward and [BLANK]. [SEP] complicated [ANSWER]\"\n",
    "\"She is happy. <|perturb|> [negation] She is [BLANK]. [SEP] unhappy [ANSWER]\",\n",
    "\"The idea is common. <|perturb|> [negation] The idea is [BLANK]. [SEP] uncommon [ANSWER]\"\n",
    "]\n",
    "\n",
    "examplesd = [\n",
    "    \"I saw nothing at all. <|perturb|> [negation] I saw [BLANK]. [SEP] something [ANSWER]\"\n",
    "    \"I found his story wholly convincing. <|perturb|> [negation] I found his story wholly [BLANK] convincing. [SEP] not [ANSWER]\"\n",
    "    \"She rarely goes out these days. <|perturb|> [negation]\tShe [BLANK] goes out these days. [SEP] never [ANSWER]\"\n",
    "    \"Bricks are made of clay in some context. <|perturb|> [negation] Bricks are made of clay in [BLANK] context. [SEP] no [ANSWER] \"\n",
    "\n",
    "]\n",
    "\n",
    "examples = [\n",
    "    \"It is possible. <|perturb|> [negation] It is [BLANK]. [SEP] impossible [ANSWER]\",\n",
    "    \"It is possible. <|perturb|> [negation] It is [BLANK]. [SEP] not possible [ANSWER]\",\n",
    "    #\"He is satisfied. <|perturb|> [negation] He is [BLANK]. [SEP] dissatisfied [ANSWER]\",\n",
    "    \"The statement is logical. <|perturb|> [negation] The statement is [BLANK]. [SEP] illogical [ANSWER]\",\n",
    "    \"The statement is logical. <|perturb|> [negation] The statement is [BLANK]. [SEP] not logical [ANSWER]\"\n",
    "    \"She is happy. <|perturb|> [negation] She is [BLANK]. [SEP] unhappy [ANSWER]\",\n",
    "    \"The idea is common. <|perturb|> [negation] The idea is [BLANK]. [SEP] uncommon [ANSWER]\",\n",
    "    #\"This action is reversible. <|perturb|> [negation] This action is [BLANK]. [SEP] irreversible [ANSWER]\",\n",
    "    #\"The argument is credible. <|perturb|> [negation] The argument is [BLANK]. [SEP] incredible [ANSWER]\",\n",
    "    #\"His behavior is responsible. <|perturb|> [negation] His behavior is [BLANK]. [SEP] irresponsible [ANSWER]\",\n",
    "    #\"The phenomenon is regular. <|perturb|> [negation] The phenomenon is [BLANK]. [SEP] irregular [ANSWER]\",\n",
    "    #\"Her decision is rational. <|perturb|> [negation] Her decision is [BLANK]. [SEP] irrational [ANSWER]\"\n",
    "]\n",
    "\n",
    "text = \"His behavior is always responsible.\"\n",
    "random_blanks = {'His behavior is [BLANK] responsible.', 'His behavior is always [BLANK].'}\n",
    "\n",
    "\n",
    "generator = PerturbationGenerator() \n",
    "\n",
    "prompt_list = generator.get_prompt_context(text,random_blanks,examples_affixal) # join the context and the inference example\n",
    "print(prompt_list)\n",
    "\n",
    "generated_sequences = generator.generate_on_prompts(prompt_list, temperature=1, num_beams=5, do_sample=True, batch_size=128, num_return_sequences=3)\n",
    "\n",
    "valid_list = generator.validate_and_sample_perturbations(generated_sequences, text, perplex_thred=100, num_perturbations=4)\n",
    "print(valid_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = generator.get_prompts(text, random_blanks)\n",
    "print(prompts)\n",
    "\n",
    "generated_sequences = generator.generate_on_prompts(prompts, temperature=1, num_beams=5, do_sample=True, batch_size=128, num_return_sequences=3)\n",
    "\n",
    "listado = generator.validate_and_sample_perturbations(generated_sequences, text, perplex_thred=10, num_perturbations=4)\n",
    "print(listado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft prompting with Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 59/59 [00:00<00:00, 3258.93 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "class TextGenerationSetup:\n",
    "\n",
    "    \"\"\"\n",
    "    Class used to setup the prompts format for training\n",
    "    \"\"\"\n",
    "    \n",
    "    PERETURB_TOK = \"<|perturb|>\"\n",
    "    BLANK_TOK = \"[BLANK]\"\n",
    "    SEP_TOK = \"[SEP]\"\n",
    "    ANSWER_TOK = \"[ANSWER]\"\n",
    "\n",
    "    def __init__(self, model_path):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_path).to(self.device)\n",
    "\n",
    "\n",
    "    def get_prompts(self,doc, blanked_sents, is_complete_blank=True):\n",
    "        prompts = []\n",
    "        for bt in blanked_sents:\n",
    "            tag = 'negation'\n",
    "            sep_tok = TextGenerationSetup.SEP_TOK if bt and is_complete_blank else \"\"\n",
    "            new_prompt = f\"{doc.strip()} {TextGenerationSetup.PERETURB_TOK} [{tag}] {bt.strip()}\".strip()\n",
    "            prompts.append(new_prompt)\n",
    "            #prompts.append(new_prompt.rstrip('.').strip())\n",
    "        return prompts\n",
    "\n",
    "    def get_answer(self,answer):\n",
    "        prompts = []\n",
    "        prompts.append(f\"{TextGenerationSetup.SEP_TOK} {answer.strip()} {TextGenerationSetup.ANSWER_TOK}\")\n",
    "        #prompts.append(answer.strip())\n",
    "        return prompts\n",
    "\n",
    "    def tokenize_function(self, examples):\n",
    "\n",
    "        input_encodings = self.tokenizer(examples['input_text'], truncation=True, padding=\"max_length\", max_length=50)\n",
    "        target_encodings = self.tokenizer(examples['target_text'], truncation=True, padding=\"max_length\", max_length=50)\n",
    "        labels = target_encodings['input_ids']\n",
    "        input_encodings[\"labels\"] = labels\n",
    "\n",
    "        return input_encodings\n",
    "    \n",
    "def process_dataframe(affixal_path, text_format ):\n",
    "\n",
    "    \"\"\"\n",
    "    Processes a DataFrame containing text data to generate a dataset suitable for text generation tasks.\n",
    "\n",
    "    This function reads a DataFrame from a pickle file, processes each row to replace specific cues with \n",
    "    a blank token, generates prompts and answers in the required format, and converts the processed data \n",
    "    into a format suitable for training a Hugging Face model.\n",
    "\n",
    "    Args:\n",
    "        affixal_path (str): Path to the pickle file containing the DataFrame with text data.\n",
    "        text_format (TextGenerationSetup): An instance of the TextGenerationSetup class used for formatting prompts and answers.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: A Hugging Face Dataset object containing the processed input and target texts.\n",
    "    \"\"\"\n",
    "\n",
    "    train_data = []\n",
    "\n",
    "    # Load the DataFrame from the pickle file\n",
    "    filtered_df = pd.read_pickle(affixal_path)\n",
    "\n",
    "    for _, row in filtered_df.iterrows():\n",
    "        text = row['text']\n",
    "        text_pos = row['text_substituted']\n",
    "        cue = row['cues'].split()[0]  # Assuming 'cues' column contains space-separated cues\n",
    "        \n",
    "        # Replace the cue in the text with '[BLANK]'\n",
    "        text_with_blank = text.replace(cue, '[BLANK]')\n",
    "        \n",
    "        # Generate the prompt and answer\n",
    "        prompt_examples = text_format.get_prompts(text_pos, [text_with_blank]) # format the input prompts\n",
    "        answer_formatted = text_format.get_answer(cue) # format the answer\n",
    "        \n",
    "        # Combine the prompt and answer in the required format\n",
    "        example = (prompt_examples[0], answer_formatted[0])\n",
    "        train_data.append(example)\n",
    "\n",
    "    # Convert to a suitable format for Hugging Face Dataset\n",
    "    train_dataset= pd.DataFrame(train_data, columns=[\"input_text\", \"target_text\"])\n",
    "    train_dataset = Dataset.from_pandas(train_dataset)\n",
    "    \n",
    "    return train_dataset\n",
    "\n",
    "# Setup the model\n",
    "model_path = \"uw-hai/polyjuice\"  \n",
    "text_gen = TextGenerationSetup(model_path)\n",
    "\n",
    "# Load affixal negations dataset in the right format for training\n",
    "affixal_df = '../data/affixal/filtered_df.pkl' # Specify the path to the pickle file\n",
    "train_dataset = process_dataframe(affixal_df,text_gen)\n",
    "tokenized_datasets = train_dataset.map(text_gen.tokenize_function, batched=True, remove_columns=[\"input_text\", \"target_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_text': 'makes for a pretty pleasant viewing experience . <|perturb|> [negation] makes for a pretty [BLANK] viewing experience .',\n",
       " 'target_text': '[SEP] unpleasant [ANSWER]'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7,680 || all params: 124,447,488 || trainable%: 0.006171277639609728\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 63/104 [04:48<03:07,  4.57s/it]\n",
      " 17%|█▋        | 4/24 [00:06<00:33,  1.67s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 158\u001b[0m\n\u001b[0;32m    155\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ModelTrainer(model\u001b[38;5;241m=\u001b[39mfoundational_model, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 158\u001b[0m trainer_module \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_virtual_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#13\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[19], line 90\u001b[0m, in \u001b[0;36mModelTrainer.train_model\u001b[1;34m(self, tokenizer, tokenized_datasets, num_virtual_tokens, num_epochs, learning_rate)\u001b[0m\n\u001b[0;32m     87\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_training_arguments(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_directory, learning_rate, num_epochs)\n\u001b[0;32m     89\u001b[0m trainer_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_trainer(peft_model, training_args, tokenized_datasets)\n\u001b[1;32m---> 90\u001b[0m \u001b[43mtrainer_prompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[0;32m     93\u001b[0m trainer_prompt\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_directory)\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\transformers\\trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\transformers\\trainer.py:1854\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1853\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1854\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1857\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1859\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1860\u001b[0m ):\n\u001b[0;32m   1861\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1862\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\transformers\\trainer.py:2735\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2732\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2735\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2738\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\transformers\\trainer.py:2758\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2756\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2757\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2758\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   2759\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2760\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\peft\\peft_model.py:1131\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[0;32m   1129\u001b[0m prompts \u001b[38;5;241m=\u001b[39m prompts\u001b[38;5;241m.\u001b[39mto(inputs_embeds\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m   1130\u001b[0m inputs_embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((prompts, inputs_embeds), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m-> 1131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1074\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1089\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:888\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    876\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    877\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    878\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    885\u001b[0m         output_attentions,\n\u001b[0;32m    886\u001b[0m     )\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    899\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:427\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    425\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    426\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[1;32m--> 427\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[0;32m    429\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:356\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    354\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_fc(hidden_states)\n\u001b[0;32m    355\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[1;32m--> 356\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    357\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\transformers\\pytorch_utils.py:108\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    107\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[1;32m--> 108\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(size_out)\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import PromptTuningConfig, TaskType, PromptTuningInit, get_peft_model\n",
    "from peft import PeftModel\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "\n",
    "class PrintLossCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Check if log history is not empty\n",
    "        if state.log_history and state.is_local_process_zero:\n",
    "            # Print the loss if it's available in the last log entry\n",
    "            if 'loss' in state.log_history[-1]:\n",
    "                print(f\"Epoch {state.epoch}: loss = {state.log_history[-1]['loss']}\")\n",
    "            else:\n",
    "                print(f\"Epoch {state.epoch}: loss not available\")\n",
    "        else:\n",
    "            print(f\"Epoch {state.epoch}: log history is empty or not accessible\")\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, model, tokenizer, working_dir=\"./\", output_dir_name=\"peft_outputs\"):\n",
    "        self.working_dir = working_dir\n",
    "        self.output_dir_name = output_dir_name\n",
    "        self.output_directory = self.create_model_directories(self.working_dir, self.output_dir_name)\n",
    "        self.model_name = \"uw-hai/polyjuice\"\n",
    "        self.foundational_model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def create_model_directories(self, base_dir, output_dir_name):\n",
    "        \"\"\"\n",
    "        Creates the base and output directories for storing models if they do not exist.\n",
    "        \n",
    "        Parameters:\n",
    "        base_dir (str): The base working directory.\n",
    "        output_dir_name (str): The name of the output directory.\n",
    "        \n",
    "        Returns:\n",
    "        str: The path to the output directory.\n",
    "        \"\"\"\n",
    "        # Create the name of the output directory\n",
    "        output_directory = os.path.join(base_dir, output_dir_name)\n",
    "        \n",
    "        # Create the base directory if it does not exist\n",
    "        if not os.path.exists(base_dir):\n",
    "            os.mkdir(base_dir)\n",
    "        \n",
    "        # Create the output directory if it does not exist\n",
    "        if not os.path.exists(output_directory):\n",
    "            os.mkdir(output_directory)\n",
    "        \n",
    "        return output_directory\n",
    "    \n",
    "    def create_training_arguments(self, path, learning_rate=0.0035, epochs=6):\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=path, # Where the model predictions and checkpoints will be written\n",
    "            #use_cpu=True, # This is necessary for CPU clusters.\n",
    "            #auto_find_batch_size=True, # Find a suitable batch size that will fit into memory automatically\n",
    "            per_device_train_batch_size=8,\n",
    "            learning_rate=learning_rate, # Higher learning rate than full Fine-Tuning\n",
    "            num_train_epochs=epochs,\n",
    "            logging_strategy=\"epoch\"\n",
    "        )\n",
    "        return training_args\n",
    "\n",
    "    def create_trainer(self, model, training_args, train_dataset):\n",
    "        trainer = Trainer(\n",
    "            model=model, # We pass in the PEFT version of the foundation model, bloomz-560M\n",
    "            args=training_args, # The args for the training.\n",
    "            train_dataset=train_dataset, # The dataset used to train the model.\n",
    "            callbacks=[PrintLossCallback()],\n",
    "            data_collator=DataCollatorForLanguageModeling(self.tokenizer, mlm=False) # mlm=False indicates not to use masked language modeling\n",
    "        )\n",
    "        return trainer\n",
    "    \n",
    "    def train_model(self, tokenizer, tokenized_datasets, num_virtual_tokens=4, num_epochs=2, learning_rate=0.003):\n",
    "        # Define prompt tuning configuration\n",
    "        peft_config = PromptTuningConfig(\n",
    "            task_type=TaskType.CAUSAL_LM, # This type indicates the model will generate text.\n",
    "            prompt_tuning_init=PromptTuningInit.RANDOM,  # The added virtual tokens are initialized with random numbers\n",
    "            num_virtual_tokens=num_virtual_tokens, # Number of virtual tokens to be added and trained.\n",
    "            tokenizer_name_or_path=self.model_name # The pre-trained model.\n",
    "        )\n",
    "        \n",
    "        peft_model = get_peft_model(self.foundational_model, peft_config)\n",
    "        print(peft_model.print_trainable_parameters())\n",
    "        \n",
    "        training_args = self.create_training_arguments(self.output_directory, learning_rate, num_epochs)\n",
    "        \n",
    "        trainer_prompt = self.create_trainer(peft_model, training_args, tokenized_datasets)\n",
    "        trainer_prompt.train()\n",
    "        \n",
    "        # Save the trained model\n",
    "        trainer_prompt.model.save_pretrained(self.output_directory)\n",
    "\n",
    "        return trainer_prompt\n",
    "    \n",
    "    #this function returns the outputs from the model received, and inputs.\n",
    "    def get_outputs(self,model,inputs,do_sample=True,num_beams=None,num_return_sequences = 3):\n",
    "        \"\"\"\n",
    "        Generates multiple sequences of text using the provided model and inputs.\n",
    "\n",
    "        Args:\n",
    "            model: The model used for generation.\n",
    "            inputs (dict): Input tensors including 'input_ids' and 'attention_mask'.\n",
    "            do_sample (bool, optional): Whether to use sampling during generation (default: True).\n",
    "            num_beams (int, optional): Number of beams for beam search. Overrides `do_sample`.\n",
    "            num_return_sequences (int, optional): Number of sequences to generate per input (default: 3).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor containing generated sequences.\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=30,\n",
    "            early_stopping=False, #if num_beams is None else True, #The model can stop before reach the max_length\n",
    "            temperature= 1,\n",
    "            num_beams=1 if num_beams is None else num_beams,\n",
    "            do_sample=num_beams is None and do_sample,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "        )\n",
    "        return outputs\n",
    "    \n",
    "    def inference (self,input_prompt,num_return_sequences = 3, num_beams = 5 ):\n",
    "        \"\"\"\n",
    "        Generate text sequences based on an input prompt using a pretrained model saved in the directory.\n",
    "\n",
    "        Args:\n",
    "            input_prompt (str): The input prompt text to generate sequences from.\n",
    "\n",
    "        Returns:\n",
    "            list: List of generated text sequences as decoded by the tokenizer, without special tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        loaded_model_prompt = PeftModel.from_pretrained(self.foundational_model,\n",
    "                                         self.output_directory,\n",
    "                                         device_map='auto',\n",
    "                                         is_trainable=False)\n",
    "        \n",
    "        input_prompt_tok = self.tokenizer(input_prompt, return_tensors=\"pt\")\n",
    "        loaded_model_prompt_outputs = self.get_outputs(loaded_model_prompt, input_prompt_tok,num_beams = num_beams,num_return_sequences = num_return_sequences)\n",
    "        result = self.tokenizer.batch_decode(loaded_model_prompt_outputs, skip_special_tokens=True)\n",
    "\n",
    "        return result\n",
    "\n",
    "##############################\n",
    "\n",
    "# Define your foundational model, tokenizer, and tokenized datasets\n",
    "foundational_model = text_gen.model\n",
    "model_name = \"uw-hai/polyjuice\"\n",
    "tokenizer = text_gen.tokenizer\n",
    "\n",
    "# Initialize the ModelTrainer class\n",
    "trainer = ModelTrainer(model=foundational_model, tokenizer=tokenizer)\n",
    "\n",
    "# Train the model\n",
    "trainer_module = trainer.train_model(tokenizer, tokenized_datasets,num_virtual_tokens=10, num_epochs=3) #13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.5717</td>\n",
       "      <td>0.002769</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.5718</td>\n",
       "      <td>0.002538</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.5023</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.4870</td>\n",
       "      <td>0.002077</td>\n",
       "      <td>4.0</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.4106</td>\n",
       "      <td>0.001846</td>\n",
       "      <td>5.0</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     loss  learning_rate  epoch  step  train_runtime  \\\n",
       "0  5.5717       0.002769    1.0     8            NaN   \n",
       "1  5.5718       0.002538    2.0    16            NaN   \n",
       "2  5.5023       0.002308    3.0    24            NaN   \n",
       "3  5.4870       0.002077    4.0    32            NaN   \n",
       "4  5.4106       0.001846    5.0    40            NaN   \n",
       "\n",
       "   train_samples_per_second  train_steps_per_second  total_flos  train_loss  \n",
       "0                       NaN                     NaN         NaN         NaN  \n",
       "1                       NaN                     NaN         NaN         NaN  \n",
       "2                       NaN                     NaN         NaN         NaN  \n",
       "3                       NaN                     NaN         NaN         NaN  \n",
       "4                       NaN                     NaN         NaN         NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a dataframe with the logging information\n",
    "df_train_log = pd.DataFrame(trainer_module.state.log_history)\n",
    "df_train_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_loss_per_epoch(df):\n",
    "    \"\"\"\n",
    "    Plot the training loss per epoch from a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing 'epoch' and 'loss' columns.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(df['epoch'], df['loss'], marker='o', linestyle='-', color='r', label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.xticks(df['epoch'])  # Set x-axis ticks to be exactly the epoch values\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_loss_per_epoch(df_train_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "c:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\peft\\peft_model.py:1180: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Her decision is rational. <|perturb|> [negation] Her decision is [BLANK]. [SEP] not [ANS', 'Her decision is rational. <|perturb|> [negation] Her decision is [BLANK]. [SEP] not enough to', 'Her decision is rational. <|perturb|> [negation] Her decision is [BLANK]. [SEP] not to go']\n"
     ]
    }
   ],
   "source": [
    "#input_prompt = \"too much of it feels unfocused and underdeveloped . [negation] too much of it feels [BLANK] and underdeveloped\"\n",
    "input_prompt = \"Her decision is rational. <|perturb|> [negation] Her decision is [BLANK]\"\n",
    "\n",
    "# To perturb with more controls,\n",
    "#random_blanks = [\"His behavior is always [BLANK]\", \" His behavior is [BLANK]\"]\n",
    "\n",
    "#input_prompt = \"His behavior is always responsible. [negation] His behavior is [BLANK]\"\n",
    "\n",
    "#input_prompt = 'makes for a pretty pleasant viewing experience. <|perturb|> [negation] makes for a pretty [BLANK] viewing experience'\n",
    "sequence = trainer.inference (input_prompt)\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His behavior is always responsible. [negation] His behavior is [BLANK] responsible. [SEP] not [ANSWER] ', 'His behavior is always responsible. [negation] His behavior is [BLANK] responsible. [SEP] never [ANSWER] ', 'His behavior is always responsible. [negation] His behavior is [BLANK] responsible. [SEP] not always [ANSWER] ']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This section is making inference with the original model\n",
    "\n",
    "def get_outputsx(model, inputs,do_sample=True,num_beams=None):\n",
    "\n",
    "    num_return_sequences = 3\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=1000,\n",
    "        #repetition_penalty=1.5, #Avoid repetition.\n",
    "        early_stopping=False, #if num_beams is None else True, #The model can stop before reach the max_length\n",
    "        temperature= 1,\n",
    "        num_beams=1 if num_beams is None else num_beams,\n",
    "        do_sample=num_beams is None and do_sample,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        #eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return outputs\n",
    "\n",
    "model_name = \"uw-hai/polyjuice\"\n",
    "foundational_model=AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "#text = \"The project is sustainable.\"\n",
    "#random_blanks = {'The project is [BLANK]', '[BLANK] project is sustainable'}\n",
    "\n",
    "#input_prompt = tokenizer(\"The man drank from the tap. <|perturb|> [negation] The man [BLANK] from the tap.\", return_tensors=\"pt\")\n",
    "input_prompt = tokenizer(\"His behavior is always responsible. [negation] His behavior is [BLANK]\", return_tensors=\"pt\")\n",
    "#input_prompt = tokenizer(\"The project is sustainable. <|perturb|> [negation] The project is [BLANK].\", return_tensors=\"pt\")\n",
    "#input_prompt = tokenizer(\"Her decision is rational. <|perturb|> [negation] Her decision is [BLANK]\", return_tensors=\"pt\")\n",
    "foundational_outputs_prompt = get_outputsx(foundational_model, input_prompt, num_beams = 5)\n",
    "\n",
    "print(tokenizer.batch_decode(foundational_outputs_prompt, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Her decision is rational. <|perturb|> [negation] Her decision is [BLANK]', 'Her decision is rational. <|perturb|> [negation] Her decision is [BLANK]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE\n",
      "[[{'generated_text': 'Her decision is rational. <|perturb|> [negation] Her decision is [BLANK] rational. [SEP] not [ANSWER] '}, {'generated_text': 'Her decision is rational. <|perturb|> [negation] Her decision is [BLANK]. [SEP] not enough [ANSWER] '}, {'generated_text': 'Her decision is rational. <|perturb|> [negation] Her decision is [BLANK]. [SEP] not enough however [ANSWER] '}], [{'generated_text': 'Her decision is rational. <|perturb|> [negation] Her decision is [BLANK] rational. [SEP] not [ANSWER] '}, {'generated_text': 'Her decision is rational. <|perturb|> [negation] Her decision is [BLANK]. [SEP] not enough [ANSWER] '}, {'generated_text': 'Her decision is rational. <|perturb|> [negation] Her decision is [BLANK]. [SEP] not enough however [ANSWER] '}]]\n",
      "[[('negation', 'Her decision is not enough.'), ('negation', 'Her decision is not rational.'), ('negation', 'Her decision is not enough however.')], [('negation', 'Her decision is not enough.'), ('negation', 'Her decision is not rational.'), ('negation', 'Her decision is not enough however.')]]\n"
     ]
    }
   ],
   "source": [
    "from incontext import PerturbationGenerator\n",
    "\n",
    "# Example usage\n",
    "text = \"Her decision is rational.\"\n",
    "random_blanks = ['Her decision is [BLANK]']*2#, '[BLANK] project is sustainable'}\n",
    "\n",
    "generator = PerturbationGenerator()\n",
    "\n",
    "prompts = generator.get_prompts(text, random_blanks)\n",
    "print(prompts)\n",
    "\n",
    "generated_sequences = generator.generate_on_prompts(prompts, temperature=1, num_beams=5, do_sample=True, batch_size=128, num_return_sequences=3)\n",
    "\n",
    "print(generated_sequences)\n",
    "\n",
    "#listado = generator.validate_and_sample_perturbations(generated_sequences, text, perplex_thred=10, num_perturbations=4)\n",
    "#print(listado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyjuice import Polyjuice\n",
    "pj = Polyjuice(model_path=\"uw-hai/polyjuice\", is_cuda=True)\n",
    "\n",
    "# Example usage\n",
    "text = \"Her decision can me more rational.\"\n",
    "random_blanks = ['Her decision is can be more [BLANK]','Her decision is can be [BLANK]']#, '[BLANK] project is sustainable'}\n",
    "\n",
    "#text = \"It is great for kids.\"\n",
    "#random_blanks =  [\"It is great for [BLANK]\"]*2\n",
    "\n",
    "#text = \"It is great for kids.\"\n",
    "#text = \"His behavior is always responsible.\"\n",
    "\n",
    "# To perturb with more controls,\n",
    "#random_blanks = [\"His behavior is always [BLANK]\", \" His behavior is [BLANK]\"]\n",
    "\n",
    "#text = \"The project is sustainable.\"\n",
    "#random_blanks = {'The project is [BLANK]', '[BLANK] project is sustainable'}\n",
    "\n",
    "perturbations = pj.perturb(\n",
    "    orig_sent=text,\n",
    "    blanked_sent= random_blanks,\n",
    "    ctrl_code=\"negation\",\n",
    "    # Customzie perplexity score. \n",
    "    perplex_thred=30,\n",
    "    # number of perturbations to return\n",
    "    num_perturbations=3,\n",
    "    # the function also takes in additional arguments for huggingface generators.\n",
    "    num_beams=5\n",
    ")\n",
    "perturbations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset with Affixal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>dev_index</th>\n",
       "      <th>text</th>\n",
       "      <th>cues</th>\n",
       "      <th>judgment</th>\n",
       "      <th>is_important</th>\n",
       "      <th>negation_type</th>\n",
       "      <th>text_substituted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>151</td>\n",
       "      <td>a big , gorgeous , sprawling swashbuckler that...</td>\n",
       "      <td>uncomplicated [14]</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>AFFIX</td>\n",
       "      <td>a big , gorgeous , sprawling swashbuckler that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>319</td>\n",
       "      <td>too much of it feels unfocused and underdevelo...</td>\n",
       "      <td>unfocused [5]</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>AFFIX</td>\n",
       "      <td>too much of it feels focused and underdeveloped .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>274</td>\n",
       "      <td>it 's hard to like a film about a guy who is u...</td>\n",
       "      <td>unlikeable [13]</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>AFFIX</td>\n",
       "      <td>it 's hard to like a film about a guy who is u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>858</td>\n",
       "      <td>the film 's welcome breeziness and some unbeli...</td>\n",
       "      <td>unbelievably [7]</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>AFFIX</td>\n",
       "      <td>the film 's welcome breeziness and some believ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>469</td>\n",
       "      <td>so unremittingly awful that labeling it a dog ...</td>\n",
       "      <td>unremittingly [1]</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>AFFIX</td>\n",
       "      <td>so remittingly awful that labeling it a dog pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  dev_index                                               text  \\\n",
       "0      6        151  a big , gorgeous , sprawling swashbuckler that...   \n",
       "1     12        319  too much of it feels unfocused and underdevelo...   \n",
       "2     17        274  it 's hard to like a film about a guy who is u...   \n",
       "3     24        858  the film 's welcome breeziness and some unbeli...   \n",
       "4     26        469  so unremittingly awful that labeling it a dog ...   \n",
       "\n",
       "                 cues  judgment is_important negation_type  \\\n",
       "0  uncomplicated [14]         1           No         AFFIX   \n",
       "1       unfocused [5]         0          Yes         AFFIX   \n",
       "2     unlikeable [13]         0          Yes         AFFIX   \n",
       "3    unbelievably [7]         1           No         AFFIX   \n",
       "4   unremittingly [1]         0           No         AFFIX   \n",
       "\n",
       "                                    text_substituted  \n",
       "0  a big , gorgeous , sprawling swashbuckler that...  \n",
       "1  too much of it feels focused and underdeveloped .  \n",
       "2  it 's hard to like a film about a guy who is u...  \n",
       "3  the film 's welcome breeziness and some believ...  \n",
       "4  so remittingly awful that labeling it a dog pr...  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to the pickle file\n",
    "affixal_path = '../data/affixal/filtered_df.pkl'\n",
    "\n",
    "# Load the DataFrame from the pickle file\n",
    "filtered_df = pd.read_pickle(affixal_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a big , gorgeous , sprawling swashbuckler that delivers its diversions in grand , uncomplicated fashion .\n",
      "a big , gorgeous , sprawling swashbuckler that delivers its diversions in grand , complicated fashion .\n"
     ]
    }
   ],
   "source": [
    "print(filtered_df.loc[0,'text'])\n",
    "print(filtered_df.loc[0,'text_substituted'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "negation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
