{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from peft import PromptTuningConfig, TaskType, PromptTuningInit, get_peft_model,PeftModel\n",
    "import config\n",
    "\n",
    "class Modelinference:\n",
    "    def __init__(self, model, tokenizer, output_dir_name):\n",
    "        self.output_directory = output_dir_name\n",
    "        self.foundational_model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "\n",
    "    #this function returns the outputs from the model received, and inputs.\n",
    "    def get_outputs(self,model,inputs,do_sample=True,num_beams=None,num_return_sequences = 3):\n",
    "        \"\"\"\n",
    "        Generates multiple sequences of text using the provided model and inputs.\n",
    "\n",
    "        Args:\n",
    "            model: The model used for generation.\n",
    "            inputs (dict): Input tensors including 'input_ids' and 'attention_mask'.\n",
    "            do_sample (bool, optional): Whether to use sampling during generation (default: True).\n",
    "            num_beams (int, optional): Number of beams for beam search. Overrides `do_sample`.\n",
    "            num_return_sequences (int, optional): Number of sequences to generate per input (default: 3).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor containing generated sequences.\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=30,\n",
    "            early_stopping=False, #if num_beams is None else True, #The model can stop before reach the max_length\n",
    "            temperature= 1,\n",
    "            num_beams=1 if num_beams is None else num_beams,\n",
    "            do_sample=num_beams is None and do_sample,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "        )\n",
    "        return outputs\n",
    "    \n",
    "    def inference (self,input_prompt,num_return_sequences = 3, num_beams = 5 ):\n",
    "        \"\"\"\n",
    "        Generate text sequences based on an input prompt using a pretrained model saved in the directory.\n",
    "\n",
    "        Args:\n",
    "            input_prompt (str): The input prompt text to generate sequences from.\n",
    "\n",
    "        Returns:\n",
    "            list: List of generated text sequences as decoded by the tokenizer, without special tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        loaded_model_prompt = PeftModel.from_pretrained(self.foundational_model,\n",
    "                                         self.output_directory,\n",
    "                                         device_map='auto',\n",
    "                                         is_trainable=False)\n",
    "        \n",
    "        input_prompt_tok = self.tokenizer(input_prompt, return_tensors=\"pt\")\n",
    "        loaded_model_prompt_outputs = self.get_outputs(loaded_model_prompt, input_prompt_tok,num_beams = num_beams,num_return_sequences = num_return_sequences)\n",
    "        result = self.tokenizer.batch_decode(loaded_model_prompt_outputs, skip_special_tokens=True)\n",
    "\n",
    "        return result\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_helper import setup_model\n",
    "\n",
    "device,tokenizer, model = setup_model(config.MODEL_PATH)\n",
    "output_directory = './peft_outputs'\n",
    "\n",
    "input_prompt = \"Her decision is rational love. <|perturb|> [negation] Her decision is [BLANK] love\"\n",
    "\n",
    "# Define your foundational model, tokenizer, and tokenized datasets\n",
    "foundational_model = model\n",
    "model_name = \"uw-hai/polyjuice\"\n",
    "\n",
    "# Initialize the ModelTrainer class\n",
    "inf_class =  Modelinference(foundational_model, tokenizer, output_directory)\n",
    "\n",
    "sequence = inf_class.inference (input_prompt)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
