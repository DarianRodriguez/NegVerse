{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyjuice import Polyjuice\n",
    "\n",
    "pj = Polyjuice(model_path=\"uw-hai/polyjuice\", is_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class \n",
    "\n",
    "init  \n",
    "        self.perplex_scorer = None\n",
    "        self.distance_scorer = None\n",
    "        self.generator = None\n",
    "        self.spacy_processor = None\n",
    "\n",
    "    def _process(self, sentence: str):\n",
    "        if not self.validate_and_load_model(\"spacy_processor\"): return None\n",
    "        return self.spacy_processor(str(sentence))\n",
    "    \n",
    "        def validate_and_load_model(self, model_name: str) -> bool:\n",
    "        \"\"\"Validate whether the generator or scorer are loaded.\n",
    "        If not, load the model.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): the identifier of the loaded part.\n",
    "                Should be [generator, perplex_scorer].\n",
    "\n",
    "        Returns:\n",
    "            bool: If the model is successfully load.\n",
    "        \"\"\"\n",
    "        if getattr(self, model_name, None):\n",
    "            return True\n",
    "        else:\n",
    "            loader = getattr(self, f\"_load_{model_name}\", None)\n",
    "            return loader and loader()\n",
    "        \n",
    "        def _load_spacy_processor(self, is_space_tokenizer: bool=False):\n",
    "        logger.info(\"Setup SpaCy processor.\")\n",
    "        self.spacy_processor = create_processor(is_space_tokenizer)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntactic Tree Similarities: [0.2727272727272727, 0.5454545454545454, 1.0, 0.18181818181818182]\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from typing import List\n",
    "\n",
    "import spacy\n",
    "import textdistance\n",
    "\n",
    "def syntactic_similarity(doc1, doc2):\n",
    "    def extract_dependencies(doc):\n",
    "        return [(token.text, token.dep_, token.head.text) for token in doc]\n",
    "    \n",
    "    deps1 = extract_dependencies(doc1)\n",
    "    deps2 = extract_dependencies(doc2)\n",
    "    \n",
    "    overlap = set(deps1).intersection(set(deps2))\n",
    "    return len(overlap) / max(len(deps1), len(deps2))\n",
    "\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "reference = \"The cat sat on the mat because it was tired.\"\n",
    "counterfactuals = [\n",
    "    \"The cat is sitting on the mat.\",\n",
    "    \"A tired cat sat on the mat.\",\n",
    "    \"The cat sat on the mat because it was tired.\",\n",
    "    \"The dog lies on the mat.\"\n",
    "]\n",
    "\n",
    "# Calculate syntactic similarities\n",
    "doc_ref = nlp(reference)\n",
    "syntactic_similarities = [syntactic_similarity(doc_ref, nlp(sentence)) for sentence in counterfactuals]\n",
    "\n",
    "\n",
    "print(f\"Syntactic Tree Similarities: {syntactic_similarities}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat sat on the mat because it was tired.\n",
      "Syntactic Tree Similarities: [0.2727272727272727, 0.0, 1.0, 0.18181818181818182]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "def syntactic_similarity(doc1, doc2):\n",
    "    \"\"\"\n",
    "    Calculate syntactic similarity between two spaCy parsed documents based on dependency structures.\n",
    "    \n",
    "    Parameters:\n",
    "    doc1 (spacy.tokens.doc.Doc): The first parsed document.\n",
    "    doc2 (spacy.tokens.doc.Doc): The second parsed document.\n",
    "    \n",
    "    Returns:\n",
    "    float: The syntactic similarity score.\n",
    "    \"\"\"\n",
    "    extract_dependencies = lambda doc: [(token.text, token.dep_, token.head.text) for token in doc]\n",
    "    \n",
    "    deps1 = extract_dependencies(doc1)\n",
    "    deps2 = extract_dependencies(doc2)\n",
    "    \n",
    "    overlap = set(deps1).intersection(set(deps2))\n",
    "    return len(overlap) / max(len(deps1), len(deps2))\n",
    "\n",
    "def evaluate_syntactic_similarity(reference, counterfactuals):\n",
    "    \"\"\"\n",
    "    Evaluate the syntactic similarity between a reference sentence and a list of counterfactual sentences.\n",
    "    \n",
    "    Parameters:\n",
    "    reference (str): The reference sentence.\n",
    "    counterfactuals (list of str): The list of counterfactual sentences.\n",
    "    \n",
    "    Returns:\n",
    "    list of float: A list of syntactic similarity scores.\n",
    "    \"\"\"\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    doc_ref = nlp(reference)\n",
    "    print(doc_ref)\n",
    "    similarities = []\n",
    "    \n",
    "    for sentence in counterfactuals:\n",
    "        doc_candidate = nlp(sentence)\n",
    "        similarity = syntactic_similarity(doc_ref , doc_candidate)\n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "# Example usage\n",
    "reference = \"The cat sat on the mat because it was tired.\"\n",
    "counterfactuals = [\n",
    "    \"The cat is sitting on the mat.\",\n",
    "    \"The dog lay on the rug because it was exhausted.\",\n",
    "    \"The cat sat on the mat because it was tired.\",\n",
    "    \"The dog lies on the mat.\"\n",
    "]\n",
    "\n",
    "similarities = evaluate_syntactic_similarity(reference, counterfactuals)\n",
    "print(f\"Syntactic Tree Similarities: {similarities}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 387kB [00:00, 95.1MB/s]                    \n",
      "2024-07-27 17:25:32 INFO: Downloaded file to C:\\Users\\daria\\stanza_resources\\resources.json\n",
      "2024-07-27 17:25:32 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-07-27 17:25:36 INFO: File exists: C:\\Users\\daria\\stanza_resources\\en\\default.zip\n",
      "2024-07-27 17:25:40 INFO: Finished downloading models and saved to C:\\Users\\daria\\stanza_resources\n",
      "2024-07-27 17:25:40 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 387kB [00:00, 128MB/s]                     \n",
      "2024-07-27 17:25:40 INFO: Downloaded file to C:\\Users\\daria\\stanza_resources\\resources.json\n",
      "2024-07-27 17:25:42 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-07-27 17:25:42 INFO: Using device: cpu\n",
      "2024-07-27 17:25:42 INFO: Loading: tokenize\n",
      "2024-07-27 17:25:42 INFO: Loading: mwt\n",
      "2024-07-27 17:25:42 INFO: Loading: pos\n",
      "2024-07-27 17:25:42 INFO: Loading: lemma\n",
      "2024-07-27 17:25:42 INFO: Loading: constituency\n",
      "2024-07-27 17:25:43 INFO: Loading: depparse\n",
      "2024-07-27 17:25:43 INFO: Loading: sentiment\n",
      "2024-07-27 17:25:43 INFO: Loading: ner\n",
      "2024-07-27 17:25:44 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies for sentence 1:\n",
      "\n",
      "Sentence: The quick brown fox jumps over the lazy dog\n",
      "Word: The, Governor: 4, Relation: det\n",
      "Word: quick, Governor: 4, Relation: amod\n",
      "Word: brown, Governor: 4, Relation: amod\n",
      "Word: fox, Governor: 5, Relation: nsubj\n",
      "Word: jumps, Governor: 0, Relation: root\n",
      "Word: over, Governor: 9, Relation: case\n",
      "Word: the, Governor: 9, Relation: det\n",
      "Word: lazy, Governor: 9, Relation: amod\n",
      "Word: dog, Governor: 5, Relation: obl\n",
      "\n",
      "Dependencies for sentence 2:\n",
      "\n",
      "Sentence: The fast brown fox leaps over the lazy dog\n",
      "Word: The, Governor: 4, Relation: det\n",
      "Word: fast, Governor: 4, Relation: amod\n",
      "Word: brown, Governor: 4, Relation: amod\n",
      "Word: fox, Governor: 5, Relation: nsubj\n",
      "Word: leaps, Governor: 0, Relation: root\n",
      "Word: over, Governor: 9, Relation: case\n",
      "Word: the, Governor: 9, Relation: det\n",
      "Word: lazy, Governor: 9, Relation: amod\n",
      "Word: dog, Governor: 5, Relation: obl\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# Download the English model\n",
    "stanza.download('en')\n",
    "\n",
    "# Initialize the English pipeline\n",
    "nlp = stanza.Pipeline('en', use_gpu=False)\n",
    "\n",
    "# Define the sentences\n",
    "sentence1 = \"The quick brown fox jumps over the lazy dog\"\n",
    "sentence2 = \"The fast brown fox leaps over the lazy dog\"\n",
    "\n",
    "def parse_sentence(nlp, sentence):\n",
    "    # Parse the sentence\n",
    "    doc = nlp(sentence)\n",
    "    return doc\n",
    "\n",
    "def print_dependencies(doc):\n",
    "    for sentence in doc.sentences:\n",
    "        print(f\"\\nSentence: {sentence.text}\")\n",
    "        for word in sentence.words:\n",
    "            print(f\"Word: {word.text}, Governor: {word.head}, Relation: {word.deprel}\")\n",
    "\n",
    "# Parse and print dependencies for sentences\n",
    "print(\"Dependencies for sentence 1:\")\n",
    "doc1 = parse_sentence(nlp, sentence1)\n",
    "print_dependencies(doc1)\n",
    "\n",
    "print(\"\\nDependencies for sentence 2:\")\n",
    "doc2 = parse_sentence(nlp, sentence2)\n",
    "print_dependencies(doc2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit distance: 1.0\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import zss\n",
    "from zss import simple_distance\n",
    "from nltk import Tree\n",
    "import os\n",
    "\n",
    "\n",
    "# Determine the user's home directory\n",
    "HOME_DIR = os.path.expanduser('~')\n",
    "\n",
    "# Define the path to stanza resources\n",
    "STANZA_MODEL_PATH = os.path.join(HOME_DIR, 'stanza_resources', 'en')\n",
    "\n",
    "def ensure_model_downloaded():\n",
    "    # Check if the model directory exists\n",
    "    if not os.path.exists(STANZA_MODEL_PATH):\n",
    "        print(\"Model not found. Downloading...\")\n",
    "        # Ensure stanza uses the correct directory for resource storage\n",
    "        stanza.download('en', verbose=False)\n",
    "    else:\n",
    "        print(\"Model already downloaded.\")\n",
    "\n",
    "\n",
    "#ensure_model_downloaded()\n",
    "\n",
    "stanza.download('en', verbose=False)\n",
    "\n",
    "# Initialize the English pipeline\n",
    "nlp = stanza.Pipeline('en', use_gpu=False)\n",
    "\n",
    "def parse_sentence(nlp, sentence):\n",
    "    # Parse the sentence using stanza\n",
    "    doc = nlp(sentence)\n",
    "    return doc\n",
    "\n",
    "def build_tree_from_dependencies(words):\n",
    "    # Build a tree from the dependency structure\n",
    "    # Create a dictionary to store words by their ID\n",
    "    word_dict = {word.id: word for word in words}\n",
    "    \n",
    "    # Create a dictionary to store the children of each word\n",
    "    children = {word.id: [] for word in words}\n",
    "    \n",
    "    # Fill in the children dictionary\n",
    "    for word in words:\n",
    "        if word.head > 0:  # head > 0 means it has a governor\n",
    "            children[word.head].append(word)\n",
    "    \n",
    "    def to_tree(word_id):\n",
    "        word = word_dict[word_id]\n",
    "        return zss.Node(word.text, [to_tree(child.id) for child in children[word_id]])\n",
    "    \n",
    "    # Find the root (usually has head = 0)\n",
    "    root_id = [word.id for word in words if word.head == 0][0]\n",
    "    return to_tree(root_id)\n",
    "\n",
    "def compute_edit_distance(tree1, tree2):\n",
    "    # Compute and return the edit distance\n",
    "    return simple_distance(tree1, tree2)\n",
    "\n",
    "def syntactic_tree_edit_distance(sentence1, sentence2):\n",
    "    # Parse sentences\n",
    "    doc1 = parse_sentence(nlp, sentence1)\n",
    "    doc2 = parse_sentence(nlp, sentence2)\n",
    "    \n",
    "    # Convert to zss nodes\n",
    "    tree1 = build_tree_from_dependencies(doc1.sentences[0].words)\n",
    "    tree2 = build_tree_from_dependencies(doc2.sentences[0].words)\n",
    "    \n",
    "    # Compute and return the edit distance\n",
    "    distance = compute_edit_distance(tree1, tree2)\n",
    "    return distance\n",
    "\n",
    "# Example usage\n",
    "sentence1 = \"The quick brown fox jumps over the lazy dog\"\n",
    "sentence2 = \"The quick brown fox runs over the lazy dog\"\n",
    "distance = syntactic_tree_edit_distance(sentence1, sentence2)\n",
    "print(f\"Edit distance: {distance}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"The\",\n",
      "      \"lemma\": \"the\",\n",
      "      \"upos\": \"DET\",\n",
      "      \"xpos\": \"DT\",\n",
      "      \"feats\": \"Definite=Def|PronType=Art\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"det\",\n",
      "      \"start_char\": 0,\n",
      "      \"end_char\": 3,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"text\": \"quick\",\n",
      "      \"lemma\": \"quick\",\n",
      "      \"upos\": \"ADJ\",\n",
      "      \"xpos\": \"JJ\",\n",
      "      \"feats\": \"Degree=Pos\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"amod\",\n",
      "      \"start_char\": 4,\n",
      "      \"end_char\": 9,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"text\": \"brown\",\n",
      "      \"lemma\": \"brown\",\n",
      "      \"upos\": \"ADJ\",\n",
      "      \"xpos\": \"JJ\",\n",
      "      \"feats\": \"Degree=Pos\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"amod\",\n",
      "      \"start_char\": 10,\n",
      "      \"end_char\": 15,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"text\": \"fox\",\n",
      "      \"lemma\": \"fox\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"nsubj\",\n",
      "      \"start_char\": 16,\n",
      "      \"end_char\": 19,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"text\": \"jumps\",\n",
      "      \"lemma\": \"jump\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"xpos\": \"VBZ\",\n",
      "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"start_char\": 20,\n",
      "      \"end_char\": 25,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 6,\n",
      "      \"text\": \"over\",\n",
      "      \"lemma\": \"over\",\n",
      "      \"upos\": \"ADP\",\n",
      "      \"xpos\": \"IN\",\n",
      "      \"head\": 9,\n",
      "      \"deprel\": \"case\",\n",
      "      \"start_char\": 26,\n",
      "      \"end_char\": 30,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 7,\n",
      "      \"text\": \"the\",\n",
      "      \"lemma\": \"the\",\n",
      "      \"upos\": \"DET\",\n",
      "      \"xpos\": \"DT\",\n",
      "      \"feats\": \"Definite=Def|PronType=Art\",\n",
      "      \"head\": 9,\n",
      "      \"deprel\": \"det\",\n",
      "      \"start_char\": 31,\n",
      "      \"end_char\": 34,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 8,\n",
      "      \"text\": \"lazy\",\n",
      "      \"lemma\": \"lazy\",\n",
      "      \"upos\": \"ADJ\",\n",
      "      \"xpos\": \"JJ\",\n",
      "      \"feats\": \"Degree=Pos\",\n",
      "      \"head\": 9,\n",
      "      \"deprel\": \"amod\",\n",
      "      \"start_char\": 35,\n",
      "      \"end_char\": 39,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 9,\n",
      "      \"text\": \"dog\",\n",
      "      \"lemma\": \"dog\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"obl\",\n",
      "      \"start_char\": 40,\n",
      "      \"end_char\": 43,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ],\n",
      "      \"misc\": \"SpaceAfter=No\"\n",
      "    }\n",
      "  ]\n",
      "]\n",
      "#################################\n",
      "[\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"The\",\n",
      "      \"lemma\": \"the\",\n",
      "      \"upos\": \"DET\",\n",
      "      \"xpos\": \"DT\",\n",
      "      \"feats\": \"Definite=Def|PronType=Art\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"det\",\n",
      "      \"start_char\": 0,\n",
      "      \"end_char\": 3,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"text\": \"quick\",\n",
      "      \"lemma\": \"quick\",\n",
      "      \"upos\": \"ADJ\",\n",
      "      \"xpos\": \"JJ\",\n",
      "      \"feats\": \"Degree=Pos\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"amod\",\n",
      "      \"start_char\": 4,\n",
      "      \"end_char\": 9,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"text\": \"brown\",\n",
      "      \"lemma\": \"brown\",\n",
      "      \"upos\": \"ADJ\",\n",
      "      \"xpos\": \"JJ\",\n",
      "      \"feats\": \"Degree=Pos\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"amod\",\n",
      "      \"start_char\": 10,\n",
      "      \"end_char\": 15,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"text\": \"fox\",\n",
      "      \"lemma\": \"fox\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"nsubj\",\n",
      "      \"start_char\": 16,\n",
      "      \"end_char\": 19,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"text\": \"runs\",\n",
      "      \"lemma\": \"run\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"xpos\": \"VBZ\",\n",
      "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"start_char\": 20,\n",
      "      \"end_char\": 24,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 6,\n",
      "      \"text\": \"over\",\n",
      "      \"lemma\": \"over\",\n",
      "      \"upos\": \"ADP\",\n",
      "      \"xpos\": \"IN\",\n",
      "      \"head\": 9,\n",
      "      \"deprel\": \"case\",\n",
      "      \"start_char\": 25,\n",
      "      \"end_char\": 29,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 7,\n",
      "      \"text\": \"the\",\n",
      "      \"lemma\": \"the\",\n",
      "      \"upos\": \"DET\",\n",
      "      \"xpos\": \"DT\",\n",
      "      \"feats\": \"Definite=Def|PronType=Art\",\n",
      "      \"head\": 9,\n",
      "      \"deprel\": \"det\",\n",
      "      \"start_char\": 30,\n",
      "      \"end_char\": 33,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 8,\n",
      "      \"text\": \"lazy\",\n",
      "      \"lemma\": \"lazy\",\n",
      "      \"upos\": \"ADJ\",\n",
      "      \"xpos\": \"JJ\",\n",
      "      \"feats\": \"Degree=Pos\",\n",
      "      \"head\": 9,\n",
      "      \"deprel\": \"amod\",\n",
      "      \"start_char\": 34,\n",
      "      \"end_char\": 38,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 9,\n",
      "      \"text\": \"dog\",\n",
      "      \"lemma\": \"dog\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"obl\",\n",
      "      \"start_char\": 39,\n",
      "      \"end_char\": 42,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ],\n",
      "      \"misc\": \"SpaceAfter=No\"\n",
      "    }\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "def parse_sentence(nlp, sentence):\n",
    "    # Parse the sentence using stanza\n",
    "    doc = nlp(sentence)\n",
    "    return doc\n",
    "\n",
    "nlp = stanza.Pipeline('en', use_gpu=False)\n",
    "sentence1 = \"The quick brown fox jumps over the lazy dog\"\n",
    "sentence2 = \"The quick brown fox runs over the lazy dog\"\n",
    "\n",
    "print(parse_sentence(nlp, sentence1))\n",
    "print(\"#################################\")\n",
    "print(parse_sentence(nlp, sentence2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "parse_sentence() missing 1 required positional argument: 'sentence'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[159], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe quick brown fox jumps over the lazy dog\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m sentence2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe quick brown fox runs over the lazy dog\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 15\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[43msyntactic_tree_edit_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(d)\n",
      "Cell \u001b[1;32mIn[159], line 7\u001b[0m, in \u001b[0;36msyntactic_tree_edit_distance\u001b[1;34m(sentence1, sentence2)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msyntactic_tree_edit_distance\u001b[39m(sentence1, sentence2):\n\u001b[0;32m      6\u001b[0m   \u001b[38;5;66;03m# Assuming you have functions to parse sentences into NLTK Tree objects\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m   tree1 \u001b[38;5;241m=\u001b[39m \u001b[43mparse_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m   tree2 \u001b[38;5;241m=\u001b[39m parse_sentence(sentence2)\n\u001b[0;32m     10\u001b[0m   distance \u001b[38;5;241m=\u001b[39m simple_distance(tree1, tree2)\n",
      "\u001b[1;31mTypeError\u001b[0m: parse_sentence() missing 1 required positional argument: 'sentence'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tree import Tree\n",
    "from zss import simple_distance\n",
    "\n",
    "def syntactic_tree_edit_distance(sentence1, sentence2):\n",
    "  # Assuming you have functions to parse sentences into NLTK Tree objects\n",
    "  tree1 = parse_sentence(sentence1)\n",
    "  tree2 = parse_sentence(sentence2)\n",
    "\n",
    "  distance = simple_distance(tree1, tree2)\n",
    "  return distance\n",
    "\n",
    "sentence1 = \"The quick brown fox jumps over the lazy dog\"\n",
    "sentence2 = \"The quick brown fox runs over the lazy dog\"\n",
    "d = syntactic_tree_edit_distance(sentence1, sentence2)\n",
    "print(d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path to the text file\n",
    "file_path = '../data/affixal/affixal_list.txt'\n",
    "\n",
    "# Read the text file into a DataFrame\n",
    "affixal_pairs = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Display the DataFrame\n",
    "affixal_pairs.head()\n",
    "\n",
    "df_subset = affixal_pairs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, 8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affixal_path = '../data/affixal/filtered_df.pkl'\n",
    "filtered_df = pd.read_pickle(affixal_path)\n",
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File path to the text file\n",
    "file_path = '../data/non_verbal/sentence_negated_modified.txt'\n",
    "\n",
    "# Read the text file into a DataFrame\n",
    "affixal_pairs = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "affixal_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading data\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from data_preprocess import TextGenerationSetup, process_dataframe, Trainer_preprocess, process_dataframe_general, process_data\n",
    "from training_helper import setup_model,create_model_directories, create_peft_model, train_engine\n",
    "from peft import PromptTuningConfig, TaskType, PromptTuningInit, get_peft_model,PeftModel\n",
    "from inference import get_outputs\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "import config\n",
    "\n",
    "def set_random_seed(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "set_random_seed(config.SEED)\n",
    "\n",
    "# Setup the model\n",
    "text_format = TextGenerationSetup(config.MODEL_PATH)\n",
    "\n",
    "# Load affixal negations dataset in the right format for training\n",
    "print(\"\\n Loading data\")\n",
    "new_affixal_path = '../data/affixal/generated_sentences.txt' \n",
    "affixal_df = '../data/affixal/filtered_df.pkl' # Specify the path to the pickle file\n",
    "new_affixal = process_data(new_affixal_path)\n",
    "\n",
    "train_dataset_affixal_1 = process_dataframe(affixal_df,text_format,True)\n",
    "train_dataset_affixal_2 = process_dataframe(affixal_df,text_format, False) # entire sentence\n",
    "train_dataset_affixal_3 = process_dataframe_general(new_affixal,text_format,True)\n",
    "train_dataset_affixal_4 = process_dataframe_general(new_affixal,text_format,False)\n",
    "train_dataset_affixal = concatenate_datasets([train_dataset_affixal_3,train_dataset_affixal_4])\n",
    "\n",
    "train_dataset_affixal_part = concatenate_datasets([train_dataset_affixal_1,train_dataset_affixal_3])\n",
    "train_dataset_affixal_complete = concatenate_datasets([train_dataset_affixal_2,train_dataset_affixal_4])\n",
    "\n",
    "\n",
    "#tokenized_datasets = train_dataset_affixal.map(text_format.tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset_affixal_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_dataset_affixal_part['input_text'][0])\n",
    "#print(train_dataset_affixal_complete['input_text'][0])\n",
    "\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "\n",
    "# Initialize an empty dataset\n",
    "new_train = Dataset.from_dict({\"input_text\": []})\n",
    "\n",
    "test_size=0.2\n",
    "random_state=42\n",
    "data = train_dataset_affixal_part\n",
    "\n",
    "indices = list(range(len(data)))\n",
    "train_indices, val_indices = train_test_split(indices, test_size=test_size, random_state=random_state)\n",
    "\n",
    "dataset = [train_dataset_affixal_part, train_dataset_affixal_complete] \n",
    "\n",
    "for data in dataset:\n",
    "    # Create training and validation datasets\n",
    "    train_dataset = data.select(train_indices)\n",
    "    val_dataset = data.select(val_indices)\n",
    "\n",
    "    new_train = concatenate_datasets([new_train, train_dataset])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "280\n",
      "The customer service representative was sympathetic to the customer's complaint. <|perturb|> [negation] The customer service representative was [BLANK] to the customer's complaint [SEP] unsympathetic [ANSWER]\n",
      "The customer service representative was sympathetic to the customer's complaint. <|perturb|> [negation] [BLANK] [SEP] The customer service representative was unsympathetic to the customer's complaint. [ANSWER]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_indices))\n",
    "print(len(new_train))\n",
    "\n",
    "x = 4\n",
    "print(new_train['input_text'][x])\n",
    "print(new_train['input_text'][x+140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_dataset(tokenized_dataset, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits the tokenized dataset into training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        tokenized_dataset (Dataset): The tokenized dataset to be split.\n",
    "        test_size (float, optional): The proportion of the dataset to include in the validation split (default is 0.2).\n",
    "        random_state (int, optional): Random seed for reproducibility (default is 42).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dataset, Dataset]: Training and validation datasets.\n",
    "    \"\"\"\n",
    "    # Convert tokenized dataset to a list of indices\n",
    "    indices = list(range(len(tokenized_dataset)))\n",
    "    \n",
    "    # Split indices into training and validation sets\n",
    "    train_indices, val_indices = train_test_split(indices, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Create training and validation datasets\n",
    "    train_dataset = tokenized_dataset.select(train_indices)\n",
    "    val_dataset = tokenized_dataset.select(val_indices)\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "train_subset, val_subset = split_dataset(tokenized_datasets, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(232, 3)\n",
      "(185, 3)\n",
      "(47, 3)\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets.shape)\n",
    "print(train_subset.shape)\n",
    "print(val_subset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "dataset_split = tokenized_datasets.train_test_split(test_size=0.2, seed=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The children's laughter was a joyful sound in the crowded room. <|perturb|> [negation] The children's laughter was a [BLANK] sound in the [BLANK] room\",\n",
       " \"The employee's work was mindful and purposeful, requiring attention and focus to complete. <|perturb|> [negation] The employee's work was [BLANK] and [BLANK]\",\n",
       " 'The presentation was informative and provided valuable insights. <|perturb|> [negation] [BLANK]',\n",
       " \"The team's discipline and focus led to timely completion of projects and improved productivity. <|perturb|> [negation] The team's [BLANK] of [BLANK] and [BLANK]\",\n",
       " 'The new policy has caused desired consequences for the employees. <|perturb|> [negation] [BLANK]',\n",
       " 'The students were interested in the lesson, and their enthusiasm was contagious. <|perturb|> [negation] [BLANK]',\n",
       " \"The city's streets were a well-organized and efficient system, with clear rules and regulations governing traffic. <|perturb|> [negation] [BLANK]\",\n",
       " \"The customer's demands were rational and reasonable. <|perturb|> [negation] [BLANK]\",\n",
       " 'The procedure was painful, and the patient felt extreme discomfort throughout. <|perturb|> [negation] [BLANK]',\n",
       " 'The new AI system was designed to be virtuous, able to perform tasks with a strong moral and ethical foundation. <|perturb|> [negation] The new AI system was designed to be [BLANK] able to perform tasks [BLANK] moral [BLANK] ethical [BLANK]',\n",
       " 'The new virus was susceptible to the standard treatment. <|perturb|> [negation] [BLANK]',\n",
       " \"The company's competitors were merciful in their willingness to collaborate. <|perturb|> [negation] [BLANK]\",\n",
       " \"The employee's behavior was normal, and their productivity increased. <|perturb|> [negation] [BLANK]\",\n",
       " \"The employee's enthusiastic attitude towards the project was evident in their zealous approach. <|perturb|> [negation] [BLANK]\",\n",
       " \"The new employee's performance seemed formed, with a clear understanding of their responsibilities and expectations. <|perturb|> [negation] The new employee's performance seemed [BLANK] with [BLANK]\",\n",
       " \"The team leader's decision to change the project timeline was met with widespread agreement from the team members. <|perturb|> [negation] The team leader's decision to change the project timeline was met with widespread [BLANK] from the team members\",\n",
       " \"The company's profits increased by 20% last quarter. <|perturb|> [negation] The company's profits [BLANK] by 20% last quarter\",\n",
       " 'The customer was polite to the salesperson, making it clear they were interested in purchasing something. <|perturb|> [negation] [BLANK]',\n",
       " \"The stock market's movements were predictable, allowing investors to make informed decisions. <|perturb|> [negation] The stock market's movements were [BLANK]\",\n",
       " 'The sound of the river was distinct, making it easy to determine its source. <|perturb|> [negation] The sound of the river was [BLANK] making it [BLANK] to determine its source',\n",
       " \"The employee's behavior was appropriate and professional. <|perturb|> [negation] [BLANK]\",\n",
       " 'The customer was polite to the salesperson, making it clear they were interested in purchasing something. <|perturb|> [negation] The customer was [BLANK] to the salesperson, making it clear they were [BLANK] interested in purchasing [BLANK]',\n",
       " 'The new virus was susceptible to the standard treatment. <|perturb|> [negation] The new virus was [BLANK] to the standard treatment',\n",
       " 'The sound of the river was distinct, making it easy to determine its source. <|perturb|> [negation] [BLANK]',\n",
       " 'The ghost was visible to the naked eye. <|perturb|> [negation] The ghost was [BLANK] to the naked eye',\n",
       " \"The employee's behavior was normal, and their productivity increased. <|perturb|> [negation] The employee's behavior was [BLANK]\",\n",
       " 'The new policy will have a beneficial impact on employee productivity. <|perturb|> [negation] The new policy will have a [BLANK] impact on employee [BLANK]',\n",
       " 'The project has been active since last year. <|perturb|> [negation] [BLANK]',\n",
       " 'The customer service representative showed emotional empathy during the heated argument. <|perturb|> [negation] The customer service representative [BLANK] during the heated argument',\n",
       " \"The new product launch was important to the company's overall strategy. <|perturb|> [negation] [BLANK]\",\n",
       " 'The new office chair was comfortable, providing relief from discomfort during long work hours. <|perturb|> [negation] [BLANK]',\n",
       " \"The team leader's decision to change the project timeline was met with widespread agreement from the team members. <|perturb|> [negation] [BLANK]\",\n",
       " 'The new policy has caused desired consequences for the employees. <|perturb|> [negation] The new policy has caused [BLANK] consequences for the employees',\n",
       " 'The legal system delivered justice to the innocent victim. <|perturb|> [negation] [BLANK]',\n",
       " \"The company's goals seemed possible given the favorable market conditions. <|perturb|> [negation] The company's goals seemed [BLANK] given the [BLANK] market conditions\",\n",
       " \"The company's actions were ethical and aligned with the values of the organization. <|perturb|> [negation] The company's actions were [BLANK] and [BLANK]\",\n",
       " \"The new employee's work was related to the company's goals. <|perturb|> [negation] The new employee's work was [BLANK] to the company's goals\",\n",
       " 'The new employee felt secure in his new role due to the comprehensive training provided. <|perturb|> [negation] [BLANK]',\n",
       " \"The company's profits increased by 20% last quarter. <|perturb|> [negation] [BLANK]\",\n",
       " \"The family's financial situation seemed promising, with a bright future ahead of them. <|perturb|> [negation] The family's financial situation seemed [BLANK] with [BLANK]\",\n",
       " 'The decision to fire the employee was logical given their poor performance. <|perturb|> [negation] The decision to fire the employee was [BLANK] given their [BLANK] performance',\n",
       " \"The company's success was independent of the performance of its employees. <|perturb|> [negation] The company's success was [BLANK] the performance of its employees\",\n",
       " \"The new employee's performance was sure, and it was clear that they would be a good fit for the company. <|perturb|> [negation] The new employee's performance was [BLANK] and it was [BLANK] they would be a good fit for the company\",\n",
       " \"The doctor was patient with the patient's lack of cooperation during the examination. <|perturb|> [negation] The doctor was [BLANK] with the patient's lack of cooperation during the examination\",\n",
       " 'The teacher gave the student a fair grade, despite their hard work. <|perturb|> [negation] [BLANK]',\n",
       " \"The manager's direct criticism of the employee's work was constructive and helpful. <|perturb|> [negation] [BLANK]\",\n",
       " 'The new policy is necessary to address the recent changes in the industry. <|perturb|> [negation] [BLANK]']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = val_subset['input_text']\n",
    "test_data = dataset_split['test']\n",
    "test_data_2 = [text.split('[SEP]')[0].strip() for text in test_data['input_text']]\n",
    "test_data_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from inference import get_outputs\n",
    "import config\n",
    "from inference import NegationModel\n",
    "\n",
    "output_directory = \"./peft_outputs\"\n",
    "# Inference\n",
    "input_prompt = \"His behavior is always responsible. <|perturb|> [negation] [BLANK]\"\n",
    "\n",
    "negation_model = NegationModel(output_directory)\n",
    "\n",
    "#create batches\n",
    "test_dataloader = DataLoader(test_data_2, batch_size=8, drop_last=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "generations_original = []\n",
    "generations_peft = []\n",
    "i = 0\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    outputs = negation_model.infer(batch, num_beams=5, num_return_sequences=3)\n",
    "    generations_peft.append(outputs[\"trained_model_outputs\"])\n",
    "    generations_original.extend(outputs[\"original_model_outputs\"])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The children's laughter was a joyful sound in the crowded room. <|perturb|> [negation] The children's laughter was a [BLANK] sound in the [BLANK] room!! [BLANK]. [SEP] non- [ANSWER] EMPTY [ANSWER], not in the hall [ANSWER] \",\n",
       " \"The children's laughter was a joyful sound in the crowded room. <|perturb|> [negation] The children's laughter was a [BLANK] sound in the [BLANK] room!! [BLANK]. [SEP] non- [ANSWER] EMPTY [ANSWER] EMPTY [ANSWER] \",\n",
       " \"The children's laughter was a joyful sound in the crowded room. <|perturb|> [negation] The children's laughter was a [BLANK] sound in the [BLANK] room!! [BLANK]. [SEP] non- [ANSWER] EMPTY [ANSWER] at recess [ANSWER] \",\n",
       " \"The employee's work was mindful and purposeful, requiring attention and focus to complete. <|perturb|> [negation] The employee's work was [BLANK] and [BLANK]!!, [BLANK] attention to complete. [SEP] unorganized [ANSWER] unorganized [ANSWER] not [ANSWER] \",\n",
       " \"The employee's work was mindful and purposeful, requiring attention and focus to complete. <|perturb|> [negation] The employee's work was [BLANK] and [BLANK]!!, [BLANK] attention to complete. [SEP] non-essential [ANSWER] unimportant [ANSWER] requiring no [ANSWER] \",\n",
       " \"The employee's work was mindful and purposeful, requiring attention and focus to complete. <|perturb|> [negation] The employee's work was [BLANK] and [BLANK]!!, [BLANK] attention to complete. [SEP] unorganized [ANSWER] unorganized [ANSWER] not requiring [ANSWER] \",\n",
       " 'The presentation was informative and provided valuable insights. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!!!!!!! [SEP] The presentation was non-existent and unorganized. [ANSWER] ',\n",
       " 'The presentation was informative and provided valuable insights. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!!!!!!! [SEP] The presentation was not very informative and did not provide much information. [ANSWER] ',\n",
       " 'The presentation was informative and provided valuable insights. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!!!!!!! [SEP] The presentation was unorganized and did not provide much information. [ANSWER] ',\n",
       " \"The team's discipline and focus led to timely completion of projects and improved productivity. <|perturb|> [negation] The team's [BLANK] of [BLANK] and [BLANK]. [SEP] focus on controlling the snake didn't help [ANSWER] the project [ANSWER] didn't help with productivity [ANSWER] \",\n",
       " \"The team's discipline and focus led to timely completion of projects and improved productivity. <|perturb|> [negation] The team's [BLANK] of [BLANK] and [BLANK]. [SEP] focus on controlling the snake didn't help [ANSWER] the project [ANSWER] couldn't help productivity [ANSWER] \",\n",
       " \"The team's discipline and focus led to timely completion of projects and improved productivity. <|perturb|> [negation] The team's [BLANK] of [BLANK] and [BLANK]. [SEP] focus on controlling the snake instead of the snake resulted in poor results [ANSWER] the results [ANSWER] in the execution of the plan [ANSWER] \",\n",
       " 'The new policy has caused desired consequences for the employees. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!!!!! [SEP] The new policy has not caused any problems for the employees. [ANSWER] ',\n",
       " 'The new policy has caused desired consequences for the employees. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!!!!! [SEP] The new policy has no effect on the employees. [ANSWER] ',\n",
       " 'The new policy has caused desired consequences for the employees. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!!!!! [SEP] The new policy has not caused any problems for the users. [ANSWER] ',\n",
       " 'The students were interested in the lesson, and their enthusiasm was contagious. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!! [SEP] The students were interested in the lesson and their enthusiasm died not. [ANSWER] ',\n",
       " 'The students were interested in the lesson, and their enthusiasm was contagious. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!! [SEP] The students were interested in the lesson, but their enthusiasm died not. [ANSWER] ',\n",
       " 'The students were interested in the lesson, and their enthusiasm was contagious. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!! [SEP] The students were interested in the lesson, but their enthusiasm was not enough. [ANSWER] ',\n",
       " \"The city's streets were a well-organized and efficient system, with clear rules and regulations governing traffic. <|perturb|> [negation] [BLANK]!!!!!!!! [SEP] The city's streets were not nearly as organized and efficient as the streets of a city, with no laws and consternation. [ANSWER] \",\n",
       " \"The city's streets were a well-organized and efficient system, with clear rules and regulations governing traffic. <|perturb|> [negation] [BLANK]!!!!!!!! [SEP] The city's streets were not nearly as organized and efficient as the streets of a great city, with no laws and consternation. [ANSWER] \",\n",
       " \"The city's streets were a well-organized and efficient system, with clear rules and regulations governing traffic. <|perturb|> [negation] [BLANK]!!!!!!!! [SEP] The city's streets were not nearly as organized and efficient as the streets of a modern city, with no laws and constable's wagons. [ANSWER] \",\n",
       " \"The customer's demands were rational and reasonable. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!!!!!!! [SEP] The customer's demands were not met and the counter demanded a price. The price was too high. [ANSWER] \",\n",
       " \"The customer's demands were rational and reasonable. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!!!!!!! [SEP] The customer's demands were not met and the counter demanded a price. The price was too low. [ANSWER] \",\n",
       " \"The customer's demands were rational and reasonable. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!!!!!!! [SEP] The customer's demands were not met and the counter demanded a price and no price. The price was too low. [ANSWER] \"]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generations_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trained_model_outputs': [\"The new employee's work was related to the company's goals. <|perturb|> [negation] The new employee's work was [BLANK] to the company's goals. [SEP] unrelated [ANSWER] \",\n",
       "  \"The new employee's work was related to the company's goals. <|perturb|> [negation] The new employee's work was [BLANK] to the company's goals. [SEP] not related [ANSWER] \",\n",
       "  \"The new employee's work was related to the company's goals. <|perturb|> [negation] The new employee's work was [BLANK] to the company's goals. [SEP] unimportant [ANSWER] \"],\n",
       " 'original_model_outputs': [\"The new employee's work was related to the company's goals. <|perturb|> [negation] The new employee's work was [BLANK] to the company's goals. [SEP] not related [ANSWER] \",\n",
       "  \"The new employee's work was related to the company's goals. <|perturb|> [negation] The new employee's work was [BLANK] to the company's goals. [SEP] unrelated [ANSWER] \",\n",
       "  \"The new employee's work was related to the company's goals. <|perturb|> [negation] The new employee's work was [BLANK] to the company's goals. [SEP] never related [ANSWER] \"]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ora = \"The new employee's work was related to the company's goals. <|perturb|> [negation] The new employee's work was [BLANK] to the company's goals.\"\n",
    "outputs2 = negation_model.infer(ora, num_beams=5, num_return_sequences=3)\n",
    "outputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The new employee's work was [BLANK] to the company's goals. [SEP] unrelated [ANSWER] \n"
     ]
    }
   ],
   "source": [
    "PERETURB_TOK = \"<|perturb|>\"\n",
    "BLANK_TOK = \"[BLANK]\"\n",
    "SEP_TOK = \"[SEP]\"\n",
    "ANSWER_TOK = \"[ANSWER]\"\n",
    "NEG_TOK = \"[negation]\"\n",
    "\n",
    "\n",
    "generations = outputs2['trained_model_outputs']\n",
    "\n",
    "for text in generations:\n",
    "    original = text.split(PERETURB_TOK)[0]\n",
    "    blanked = text.split(NEG_TOK)[-1]\n",
    "\n",
    "\n",
    "    print(blanked)\n",
    "    break\n",
    "\n",
    "\n",
    " The new employee's work was [BLANK] to the company's goals [BLANK]. [SEP] unrelated [ANSWER] tomorrow [ANSWER]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The new policy is unnecessary to address the recent changes in the industry.\n",
      " the new policy is unnecessary to address the recent changes in the industry.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "input_str = \"<|perturb|> [negation] The new employee's work was [BLANK] to the company's goals [BLANK]. [SEP] unrelated [ANSWER] tomorrow [ANSWER]\"\n",
    "#input_str = \"The new employee's work was related to the company's goals. <|perturb|> [negation] The new employee's work was [BLANK] to the company's goals. [SEP] unrelated [ANSWER] \"\n",
    "#input_str = 'The new policy has caused desired consequences for the employees. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!!!!! [SEP] The new policy has no effect on the employees. [ANSWER] '\n",
    "input_str = 'The new policy is necessary to address the recent changes in the industry. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! [SEP] The new policy is unnecessary to address the recent changes in the industry. [ANSWER]'\n",
    "\n",
    "EMPTY_TOK = \"[EMPTY]\"\n",
    "\n",
    "def remove_blanks(total_sequence):\n",
    "    try:\n",
    "        text = total_sequence.split(NEG_TOK)[-1]\n",
    "        before, answers = text.split(SEP_TOK)\n",
    "        answers = [x.strip() for x in answers.split(ANSWER_TOK)][:-1]\n",
    "        answers = [x if x != EMPTY_TOK else '' for x in answers]\n",
    "        for a in answers:\n",
    "            if a == '':\n",
    "                before = re.sub(r' %s' % re.escape(BLANK_TOK), a, before, count=1)\n",
    "            else:\n",
    "                before = re.sub(r'%s' % re.escape(BLANK_TOK), a, before, count=1)\n",
    "            final = before.split('!')[0]\n",
    "        return final, answers\n",
    "    except:\n",
    "        return text, []\n",
    "\n",
    "\n",
    "    \n",
    "normalized, _ = remove_blanks(input_str)\n",
    "print(normalized)\n",
    "print(normalized.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "range(0, 24)\n"
     ]
    }
   ],
   "source": [
    "f = range(len(generations_original))\n",
    "\n",
    "print(len(generations_original))\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The new employee's work was unrelated to the company's goals. \n",
      "The new employee's work was related to the company's goals. \n"
     ]
    }
   ],
   "source": [
    "generations = outputs2['trained_model_outputs']\n",
    "\n",
    "for text in generations:\n",
    "    original = text.split(PERETURB_TOK)[0]\n",
    "    replaced_text,_ = remove_blanks(text)\n",
    "    \n",
    "    break\n",
    "\n",
    "print(replaced_text)\n",
    "print(original )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new employee's work was unrelated to the company's goals tomorrow [ANSWER].\n"
     ]
    }
   ],
   "source": [
    "def fill_in_the_blanks(input_str: str) -> str:\n",
    "    # Split the input string by '[SEP]'\n",
    "    template, answers_str = input_str.split(' [SEP] ')\n",
    "    \n",
    "    # Extract the words to fill the blanks from the answers string\n",
    "    answers = answers_str.split(' [ANSWER] ')\n",
    "    \n",
    "    # Split the template into parts where blanks are present\n",
    "    template_parts = template.split('[BLANK]')\n",
    "    \n",
    "    # Combine the template parts and answers to form the complete sentence\n",
    "    complete_sentence = template_parts[0]\n",
    "    for i in range(len(answers)):\n",
    "        complete_sentence += answers[i] + template_parts[i + 1]\n",
    "    \n",
    "    return complete_sentence\n",
    "\n",
    "# Example usage\n",
    "input_str = \"The new employee's work was [BLANK] to the company's goals [BLANK]. [SEP] unrelated [ANSWER] tomorrow [ANSWER]\"\n",
    "filled_sentence = fill_in_the_blanks(input_str)\n",
    "print(filled_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The new AI system was designed to be virtuous, able to perform tasks with a strong moral and ethical foundation. <|perturb|> [negation] [BLANK]!!!!!!! [SEP] The new AI system was designed to be obedient, unable to perform tasks with a strong moral and ethical foundation. [ANSWER] ', 'The new AI system was designed to be virtuous, able to perform tasks with a strong moral and ethical foundation. <|perturb|> [negation] [BLANK]!!!!!!! [SEP] The new AI system was designed to be unintelligent, unable to perform tasks with a strong moral and ethical foundation. [ANSWER] ', 'The new AI system was designed to be virtuous, able to perform tasks with a strong moral and ethical foundation. <|perturb|> [negation] [BLANK]!!!!!!! [SEP] The new AI system was designed to be unreliable, unable to perform tasks with a strong moral and ethical foundation. [ANSWER] ', \"The new employee's work was related to the company's goals. <|perturb|> [negation] The new employee's work was [BLANK] to the company's goals!!!!! [SEP] unrelated [ANSWER] \", \"The new employee's work was related to the company's goals. <|perturb|> [negation] The new employee's work was [BLANK] to the company's goals!!!!! [SEP] not related [ANSWER] \", \"The new employee's work was related to the company's goals. <|perturb|> [negation] The new employee's work was [BLANK] to the company's goals!!!!! [SEP] unimportant [ANSWER] \", \"The future of the company remained certain given the company's strong financial position. <|perturb|> [negation] The future of the company remained [BLANK] the [BLANK]!!! [SEP] uncertain given [ANSWER] company's weak financial position. [ANSWER] \", \"The future of the company remained certain given the company's strong financial position. <|perturb|> [negation] The future of the company remained [BLANK] the [BLANK]!!! [SEP] uncertain given [ANSWER] company's strong financial position. [ANSWER] \", \"The future of the company remained certain given the company's strong financial position. <|perturb|> [negation] The future of the company remained [BLANK] the [BLANK]!!! [SEP] uncertain given [ANSWER] firm's strong financial position. [ANSWER] \", \"The doctor was patient with the patient's lack of cooperation during the examination. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!! [SEP] The doctor was patient with the patient's lack of cooperation during the examination. [ANSWER] \", \"The doctor was patient with the patient's lack of cooperation during the examination. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!! [SEP] The doctor was patient with the patient's lack of cooperation during the procedure. [ANSWER] \", \"The doctor was patient with the patient's lack of cooperation during the examination. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!! [SEP] The doctor was patient with the patient's lack of cooperation during the examination. [ANSWER] \", 'The new policy will be convenient for many employees. <|perturb|> [negation] The new policy will be [BLANK] for many employees!!!!!!!!!!! [SEP] inconvenient [ANSWER] ', 'The new policy will be convenient for many employees. <|perturb|> [negation] The new policy will be [BLANK] for many employees!!!!!!!!!!! [SEP] not convenient [ANSWER] ', 'The new policy will be convenient for many employees. <|perturb|> [negation] The new policy will be [BLANK] for many employees!!!!!!!!!!! [SEP] unappealing [ANSWER] ', 'The information provided was accurate and led to correct conclusions. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!!!!! [SEP] The information provided was inaccurate and did not lead to correct conclusions. [ANSWER] ', 'The information provided was accurate and led to correct conclusions. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!!!!! [SEP] The information provided was inaccurate and led to incorrect conclusions. [ANSWER] ', 'The information provided was accurate and led to correct conclusions. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!!!!! [SEP] The information provided was inaccurate and did not lead to correct conclusions. [ANSWER] ', \"The team's discipline and focus led to timely completion of projects and improved productivity. <|perturb|> [negation] The team's [BLANK] of [BLANK] and [BLANK] productivity. [SEP] lack of discipline [ANSWER] lack [ANSWER] lack [ANSWER] \", \"The team's discipline and focus led to timely completion of projects and improved productivity. <|perturb|> [negation] The team's [BLANK] of [BLANK] and [BLANK] productivity [SEP] lack of discipline [ANSWER] lack [ANSWER] lack [ANSWER] \", \"The team's discipline and focus led to timely completion of projects and improved productivity. <|perturb|> [negation] The team's [BLANK] of [BLANK] and [BLANK]. [SEP] lack of discipline [ANSWER] lack [ANSWER] lack of focus led to delays [ANSWER] \", 'The new policy is wanted by many employees. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!!!!!!! [SEP] The new policy is not wanted by many employees. [ANSWER] ', 'The new policy is wanted by many employees. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!!!!!!! [SEP] The new policy is not implemented by many employees. [ANSWER] ', 'The new policy is wanted by many employees. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!!!!!!! [SEP] The new policy is not available to many employees. [ANSWER] ']\n",
      "['The new AI system was designed to be virtuous, able to perform tasks with a strong moral and ethical foundation. <|perturb|> [negation] [BLANK]!!!!!!! [SEP] The new AI system, which was designed to be fully activated by the user, could not be programmed to be sentient. [ANSWER] ', 'The new AI system was designed to be virtuous, able to perform tasks with a strong moral and ethical foundation. <|perturb|> [negation] [BLANK]!!!!!!! [SEP] The new AI system, which was designed to be fully activated by the player, could not be programmed. [ANSWER] ', 'The new AI system was designed to be virtuous, able to perform tasks with a strong moral and ethical foundation. <|perturb|> [negation] [BLANK]!!!!!!! [SEP] The new AI system, which was designed to be fully activated by the user, could not be programmed. [ANSWER] ', \"The new employee's work was related to the company's goals. <|perturb|> [negation] The new employee's work was [BLANK] to the company's goals!!!!! [BLANK]. [SEP] not related [ANSWER] at all [ANSWER] \", \"The new employee's work was related to the company's goals. <|perturb|> [negation] The new employee's work was [BLANK] to the company's goals!!!!! [BLANK]. [SEP] not related [ANSWER] yet [ANSWER] \", \"The new employee's work was related to the company's goals. <|perturb|> [negation] The new employee's work was [BLANK] to the company's goals!!!!! [BLANK]. [SEP] unrelated [ANSWER] yet the company was focused on expanding [ANSWER] \", \"The future of the company remained certain given the company's strong financial position. <|perturb|> [negation] The future of the company remained [BLANK] the [BLANK]!!! [SEP] unclear to [ANSWER] average equity to double, around 3. 2 %. [ANSWER] \", \"The future of the company remained certain given the company's strong financial position. <|perturb|> [negation] The future of the company remained [BLANK] the [BLANK]!!! [SEP] unclear to [ANSWER] average equity to double, about 3. 2 %. [ANSWER] \", \"The future of the company remained certain given the company's strong financial position. <|perturb|> [negation] The future of the company remained [BLANK] the [BLANK]!!! [SEP] uncertain for longer than [ANSWER] company's strong foundations had been built on. [ANSWER] \", \"The doctor was patient with the patient's lack of cooperation during the examination. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!! [SEP] The doctor was patient with the patient's trust issues and not the problems with the problems with the trust issues. [ANSWER] \", \"The doctor was patient with the patient's lack of cooperation during the examination. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!! [SEP] The doctor was patient with the patient's trust issues and not the problems with the trust issues because the problems were not serious. [ANSWER] \", \"The doctor was patient with the patient's lack of cooperation during the examination. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!! [SEP] The doctor was patient with the condition but not Lawrence because Lawrence had complete control of the examination. [ANSWER] \", 'The new policy will be convenient for many employees. <|perturb|> [negation] The new policy will be [BLANK] for many employees!!!!!!!!!!! [BLANK]. [SEP] not be available [ANSWER] yet [ANSWER] ', 'The new policy will be convenient for many employees. <|perturb|> [negation] The new policy will be [BLANK] for many employees!!!!!!!!!!! [BLANK]. [SEP] not available [ANSWER] yet [ANSWER] ', 'The new policy will be convenient for many employees. <|perturb|> [negation] The new policy will be [BLANK] for many employees!!!!!!!!!!! [BLANK]. [SEP] not be used [ANSWER] yet [ANSWER] ', 'The information provided was accurate and led to correct conclusions. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!!!!! [SEP] The information provided was not accurate and did not lead to any conclusions. [ANSWER] ', 'The information provided was accurate and led to correct conclusions. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!!!!! [SEP] The information provided was not accurate and not supported by the documentation. The documentation was not reliable. [ANSWER] ', 'The information provided was accurate and led to correct conclusions. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!!!!! [SEP] The information provided was not accurate and not supported by the facts presented. [ANSWER] ', \"The team's discipline and focus led to timely completion of projects and improved productivity. <|perturb|> [negation] The team's [BLANK] of [BLANK] and [BLANK]. [SEP] focus on controlling the snake didn't help [ANSWER] the project [ANSWER] didn't help with productivity [ANSWER] \", \"The team's discipline and focus led to timely completion of projects and improved productivity. <|perturb|> [negation] The team's [BLANK] of [BLANK] and [BLANK]. [SEP] focus on controlling the snake didn't help [ANSWER] the project [ANSWER] couldn't help productivity [ANSWER] \", \"The team's discipline and focus led to timely completion of projects and improved productivity. <|perturb|> [negation] The team's [BLANK] of [BLANK] and [BLANK]. [SEP] focus on controlling the snake instead of the snake resulted in poor results [ANSWER] the results [ANSWER] in the execution of the plan [ANSWER] \", 'The new policy is wanted by many employees. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!!!!!!! [SEP] The new policy is not implemented by all employees. [ANSWER] ', 'The new policy is wanted by many employees. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!!!!!!! [SEP] The new policy is not implemented by many employees. [ANSWER] ', 'The new policy is wanted by many employees. <|perturb|> [negation] [BLANK]!!!!!!!!!!!!!!!!!!!! [SEP] The new policy is not implemented by any of the employees. [ANSWER] ']\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "#test_data_2[:4]\n",
    "print(outputs[\"trained_model_outputs\"])\n",
    "print(outputs[\"original_model_outputs\"])\n",
    "print(len(generations_original))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The new AI system was designed to be virtuous, able to perform tasks with a strong moral and ethical foundation. <|perturb|> [negation] [BLANK] [SEP] The new AI system, which was designed to be fully activated by the user, could not be programmed to be sentient. [ANSWER] ', 'The new AI system was designed to be virtuous, able to perform tasks with a strong moral and ethical foundation. <|perturb|> [negation] [BLANK] [SEP] The new AI system, which was designed to be fully activated by the player, could not be programmed. [ANSWER] ', 'The new AI system was designed to be virtuous, able to perform tasks with a strong moral and ethical foundation. <|perturb|> [negation] [BLANK] [SEP] The new AI system, which was designed to be fully activated by the user, could not be programmed. [ANSWER] ']\n",
      "['The new AI system was designed to be virtuous, able to perform tasks with a strong moral and ethical foundation. <|perturb|> [negation] [BLANK] [SEP] The new AI system was designed to be unintelligent, unable to perform tasks with a strong moral and ethical foundation. [ANSWER] ', 'The new AI system was designed to be virtuous, able to perform tasks with a strong moral and ethical foundation. <|perturb|> [negation] [BLANK] [SEP] The new AI system was designed to be obedient, unable to perform tasks with a strong moral and ethical foundation. [ANSWER] ', 'The new AI system was designed to be virtuous, able to perform tasks with a strong moral and ethical foundation. <|perturb|> [negation] [BLANK] [SEP] The new AI system was designed to be unreliable, unable to perform tasks with a strong moral and ethical foundation. [ANSWER] ']\n"
     ]
    }
   ],
   "source": [
    "print(generations_original[0])\n",
    "print(generations_peft[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Model Outputs:\n",
      "[\"The new employee's work was related to the company's goals. <|perturb|> [negation] The new employee's work was [BLANK] to the company's goals [SEP] unrelated [ANSWER] \", \"The new employee's work was related to the company's goals. <|perturb|> [negation] The new employee's work was [BLANK] to the company's goals [SEP] not related [ANSWER] \", \"The new employee's work was related to the company's goals. <|perturb|> [negation] The new employee's work was [BLANK] to the company's goals. [SEP] unrelated [ANSWER] \"]\n",
      "\n",
      "Original Model Outputs:\n",
      "[\"The new employee's work was related to the company's goals. <|perturb|> [negation] The new employee's work was [BLANK] to the company's goals. [SEP] not related [ANSWER] \", \"The new employee's work was related to the company's goals. <|perturb|> [negation] The new employee's work was [BLANK] to the company's goals. [SEP] unrelated [ANSWER] \", \"The new employee's work was related to the company's goals. <|perturb|> [negation] The new employee's work was [BLANK] to the company's goals. [SEP] never related [ANSWER] \"]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for batchs in test_dataloader:\n",
    "\n",
    "    for batch in batchs:\n",
    "\n",
    "        i += 1\n",
    "        if i ==2:\n",
    "            outputs = negation_model.infer(batch, num_beams=5, num_return_sequences=3)\n",
    "\n",
    "            print(\"Trained Model Outputs:\")\n",
    "            print(outputs[\"trained_model_outputs\"])\n",
    "            print(\"\\nOriginal Model Outputs:\")\n",
    "            print(outputs[\"original_model_outputs\"])\n",
    "\n",
    "            break\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The new AI system was designed to be virtuous, able to perform tasks with a strong moral and ethical foundation. <|perturb|> [negation] [BLANK] [SEP] The new AI system was designed to be unintelligent, unable to perform tasks with a strong moral and ethical foundation. [ANSWER] ',\n",
       " 'The new AI system was designed to be virtuous, able to perform tasks with a strong moral and ethical foundation. <|perturb|> [negation] [BLANK] [SEP] The new AI system was designed to be obedient, unable to perform tasks with a strong moral and ethical foundation. [ANSWER] ',\n",
       " 'The new AI system was designed to be virtuous, able to perform tasks with a strong moral and ethical foundation. <|perturb|> [negation] [BLANK] [SEP] The new AI system was designed to be unreliable, unable to perform tasks with a strong moral and ethical foundation. [ANSWER] ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"trained_model_outputs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The new AI system was designed to be virtuous, able to perform tasks with a strong moral and ethical foundation. <|perturb|> [negation] [BLANK] [SEP] The new AI system, which was designed to be fully activated by the user, could not be programmed to be sentient. [ANSWER] ',\n",
       " 'The new AI system was designed to be virtuous, able to perform tasks with a strong moral and ethical foundation. <|perturb|> [negation] [BLANK] [SEP] The new AI system, which was designed to be fully activated by the player, could not be programmed. [ANSWER] ',\n",
       " 'The new AI system was designed to be virtuous, able to perform tasks with a strong moral and ethical foundation. <|perturb|> [negation] [BLANK] [SEP] The new AI system, which was designed to be fully activated by the user, could not be programmed. [ANSWER] ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"original_model_outputs\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The new employee felt secure in his new role due to the comprehensive training provided. <|perturb|> [negation] The new employee felt [BLANK] in his new role due to [BLANK] [SEP] insecure [ANSWER] lack of experience. [ANSWER]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randint(0,100)\n",
    "train_dataset_affixal_3['input_text'][x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The new employee felt secure in his new role due to the comprehensive training provided. <|perturb|> [negation] [BLANK] [SEP] The new employee felt insecure in his new role due to lack of experience. [ANSWER]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_affixal_4['input_text'][x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 14.47it/s]\n",
      "Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 43\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sequences\n\u001b[0;32m     42\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI liked \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBreaking Bad\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBand of Brothers\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Do you have any recommendations of other shows I might like?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 43\u001b[0m se \u001b[38;5;241m=\u001b[39m \u001b[43mget_llama_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 29\u001b[0m, in \u001b[0;36mget_llama_response\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_llama_response\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m    Generate a response from the Llama model.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m        None: Prints the model's response.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     sequences \u001b[38;5;241m=\u001b[39m \u001b[43mllama_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sequences[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sequences\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:208\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    168\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;124;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\transformers\\pipelines\\base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1134\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1137\u001b[0m         )\n\u001b[0;32m   1138\u001b[0m     )\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\transformers\\pipelines\\base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1146\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1147\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1148\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\transformers\\pipelines\\base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1045\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1046\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1047\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:271\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    268\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[0;32m    270\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[1;32m--> 271\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    272\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\transformers\\generation\\utils.py:1764\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1756\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1757\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1758\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   1759\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1761\u001b[0m     )\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[1;32m-> 1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[0;32m   1765\u001b[0m         input_ids,\n\u001b[0;32m   1766\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1767\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mlogits_warper,\n\u001b[0;32m   1768\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1769\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[0;32m   1770\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[0;32m   1771\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[0;32m   1772\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1773\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   1774\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   1775\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1776\u001b[0m     )\n\u001b[0;32m   1778\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[0;32m   1779\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   1781\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1782\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1787\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   1788\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\transformers\\generation\\utils.py:2861\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2858\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2860\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2861\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[0;32m   2862\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2863\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2864\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   2865\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2866\u001b[0m )\n\u001b[0;32m   2868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2869\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\accelerate\\hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1181\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1178\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1181\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1193\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1068\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1059\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m   1060\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1065\u001b[0m         use_cache,\n\u001b[0;32m   1066\u001b[0m     )\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1068\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1077\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\accelerate\\hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:796\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[0;32m    793\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    795\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 796\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    797\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    798\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    799\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    800\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    801\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    802\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    804\u001b[0m )\n\u001b[0;32m    805\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    807\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\accelerate\\hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:739\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    736\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    737\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[1;32m--> 739\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mo_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m, past_key_value\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\accelerate\\hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "KEY = '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'\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,token=KEY)\n",
    "\n",
    "llama_pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    token=KEY\n",
    ")\n",
    "\n",
    "def get_llama_response(prompt: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate a response from the Llama model.\n",
    "\n",
    "    Parameters:\n",
    "        prompt (str): The user's input/question for the model.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the model's response.\n",
    "    \"\"\"\n",
    "    sequences = llama_pipeline(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=256,\n",
    "    )\n",
    "    print(\"Chatbot:\", sequences[0]['generated_text'])\n",
    "    return sequences\n",
    "\n",
    "\n",
    "\n",
    "prompt = 'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n'\n",
    "se = get_llama_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding token: None\n",
      "Padding token ID: None\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"uw-hai/polyjuice\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Print the padding token and its ID\n",
    "print(f\"Padding token: {tokenizer.pad_token}\")\n",
    "print(f\"Padding token ID: {tokenizer.pad_token_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of sentences using the negative word:\n",
      "['Unfortunately, I am unable to attend the meeting today due to a scheduling conflict.', 'The company is unable to offer a discount on the product at this time.']\n",
      "\n",
      "List of sentences using the positive word:\n",
      "['Fortunately, I am able to attend the meeting today without any scheduling conflicts.', 'The company is able to offer a discount on the product for a limited time.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Sample response\n",
    "response = \"\"\"\n",
    "Of course! I'd be happy to help. Here are two neutral and contextually appropriate sentences using the word \"unable\":\n",
    "\n",
    "1. Unfortunately, I am unable to attend the meeting today due to a scheduling conflict.\n",
    "2. The company is unable to offer a discount on the product at this time.\n",
    "\n",
    "And here are the same sentences but with the word \"able\":\n",
    "\n",
    "1. Fortunately, I am able to attend the meeting today without any scheduling conflicts.\n",
    "2. The company is able to offer a discount on the product for a limited time.\n",
    "\n",
    "I hope these sentences are helpful! Let me know if you have any other questions.\n",
    "\"\"\"\n",
    "\n",
    "def extract_sentences(response):\n",
    "    # Define regex patterns to match the sentences for both keywords\n",
    "    neg_pattern = re.compile(\n",
    "        r'Here are two neutral and contextually appropriate sentences using the word \"(\\w+)\":\\s*((?:\\d+\\.\\s.*(?:\\n|$))*)\\n+And here are the same sentences',\n",
    "        re.DOTALL\n",
    "    )\n",
    "    \n",
    "    pos_pattern = re.compile(\n",
    "        r'And here are the same sentences but with the word \"(\\w+)\":\\s*((?:\\d+\\.\\s.*(?:\\n|$))*)',\n",
    "        re.DOTALL\n",
    "    )\n",
    "    \n",
    "    # Extract negative sentences\n",
    "    neg_match = neg_pattern.search(response)\n",
    "    neg_sentences = []\n",
    "    if neg_match:\n",
    "        # Extract and filter negative sentences\n",
    "        neg_sentences_raw = neg_match.group(2).strip()\n",
    "        neg_sentences = [re.sub(r'^\\d+\\.\\s+', '', s.strip()) for s in neg_sentences_raw.split('\\n') if re.match(r'^\\d+\\.', s)]\n",
    "    \n",
    "    # Extract positive sentences\n",
    "    pos_match = pos_pattern.search(response)\n",
    "    pos_sentences = []\n",
    "    if pos_match:\n",
    "        # Extract and filter positive sentences\n",
    "        pos_sentences_raw = pos_match.group(2).strip()\n",
    "        pos_sentences = [re.sub(r'^\\d+\\.\\s+', '', s.strip()) for s in pos_sentences_raw.split('\\n') if re.match(r'^\\d+\\.', s)]\n",
    "\n",
    "    return neg_sentences, pos_sentences\n",
    "\n",
    "# Extract sentences\n",
    "negative_sentences, positive_sentences = extract_sentences(response)\n",
    "\n",
    "# Print results\n",
    "print(\"List of sentences using the negative word:\")\n",
    "print(negative_sentences)\n",
    "\n",
    "print(\"\\nList of sentences using the positive word:\")\n",
    "print(positive_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of sentences using the negative word:\n",
      "['Unfortunately, I am unable to attend the meeting today due to a scheduling conflict.', 'The company is unable to offer a discount on the product at this time.']\n",
      "\n",
      "List of sentences using the positive word:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Sample response\n",
    "response = \"\"\"\n",
    "Of course! I'd be happy to help. Here are two neutral and contextually appropriate sentences using the word \"unable\":\n",
    "\n",
    "1. Unfortunately, I am unable to attend the meeting today due to a scheduling conflict.\n",
    "2. The company is unable to offer a discount on the product at this time.\n",
    "\n",
    "And here are the same sentences but with the word \"able\", keeping minimal changes:\n",
    "\n",
    "1. Fortunately, I am able to attend the meeting today without any scheduling conflicts.\n",
    "2. The company is able to offer a discount on the product for a limited time.\n",
    "\n",
    "I hope these sentences are helpful! Let me know if you have any other questions.\n",
    "\"\"\"\n",
    "\n",
    "def extract_sentences(response):\n",
    "    # Define regex patterns to match the sentences for both keywords\n",
    "    neg_pattern = re.compile(\n",
    "        r'Here are two neutral and contextually appropriate sentences using the word \"(\\w+)\":\\s*((?:\\d+\\.\\s.*(?:\\n|$))*)\\n+And here are the same sentences',\n",
    "        re.DOTALL\n",
    "    )\n",
    "    \n",
    "    pos_pattern = re.compile(\n",
    "        r'And here are the same sentences but with the word \"(\\w+)\" instead, with minimal changes:\\s*((?:\\d+\\.\\s.*(?:\\n|$))*)\\n*I hope these sentences',\n",
    "        re.DOTALL\n",
    "    )\n",
    "    \n",
    "    # Extract negative sentences\n",
    "    neg_match = neg_pattern.search(response)\n",
    "    neg_sentences = []\n",
    "    if neg_match:\n",
    "        # Extract and filter negative sentences\n",
    "        neg_sentences_raw = neg_match.group(2).strip()\n",
    "        neg_sentences = [re.sub(r'^\\d+\\.\\s+', '', s.strip()) for s in neg_sentences_raw.split('\\n') if re.match(r'^\\d+\\.', s)]\n",
    "    \n",
    "    # Extract positive sentences\n",
    "    pos_match = pos_pattern.search(response)\n",
    "    pos_sentences = []\n",
    "    if pos_match:\n",
    "        # Extract and filter positive sentences\n",
    "        pos_sentences_raw = pos_match.group(2).strip()\n",
    "        pos_sentences = [re.sub(r'^\\d+\\.\\s+', '', s.strip()) for s in pos_sentences_raw.split('\\n') if re.match(r'^\\d+\\.', s)]\n",
    "\n",
    "    return neg_sentences, pos_sentences\n",
    "\n",
    "# Extract sentences\n",
    "negative_sentences, positive_sentences = extract_sentences(full_response)\n",
    "\n",
    "# Print results\n",
    "print(\"List of sentences using the negative word:\")\n",
    "print(negative_sentences)\n",
    "\n",
    "print(\"\\nList of sentences using the positive word:\")\n",
    "print(positive_sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant. You generate neutral, informative, and contextually appropriate sentences. Create two neutral and contextually appropriate sentences using the word 'iresponsible'. Then, provide the same sentences but with the word'responsible', keeping minimal changes. Assistant:  \n",
      "\n",
      "I am responsible for my actions.  \n",
      "I am responsible for my actions.  \n",
      "\n",
      "A:\n",
      "\n",
      "I am responsible for my actions.\n",
      "\n",
      "This is a perfectly acceptable sentence.\n",
      "\n",
      "I am responsible for my actions.\n",
      "\n",
      "This is a perfectly acceptable sentence.\n",
      "\n",
      "I am responsible for my actions.\n",
      "\n",
      "This is a perfectly acceptable sentence.\n",
      "\n",
      "I am responsible for my actions.\n",
      "\n",
      "This is a perfectly acceptable sentence.\n",
      "\n",
      "I am responsible for my actions.\n",
      "\n",
      "This is a perfectly acceptable sentence.\n",
      "\n",
      "I am responsible for my actions.\n",
      "\n",
      "This is a perfectly acceptable sentence.\n",
      "\n",
      "I am responsible for my actions.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'EleutherAI/gpt-neo-2.7B'  # A powerful model from EleutherAI\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Define inputs\n",
    "neg_word = 'iresponsible'\n",
    "pos_word = 'responsible'\n",
    "pre_prompt = \"You are a helpful assistant. You generate neutral, informative, and contextually appropriate sentences.\"\n",
    "prompt_input = (f\"Create two neutral and contextually appropriate sentences using the word '{neg_word}'. \"\n",
    "                f\"Then, provide the same sentences but with the word '{pos_word}', keeping minimal changes.\")\n",
    "\n",
    "# Construct the full prompt\n",
    "prompt = f\"{pre_prompt} {prompt_input} Assistant: \"\n",
    "\n",
    "# Encode and generate response\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "outputs = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    max_length=200,  # Adjust as needed\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.2,\n",
    "    num_return_sequences=1  # Number of responses to generate\n",
    ")\n",
    "\n",
    "# Decode and print response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts\n",
    "neg_word = 'iresponsible'\n",
    "pos_word = 'responsible'\n",
    "pre_prompt = \"You are a helpful assistant. You generate neutral, informative, and contextually appropriate sentences.\"\n",
    "prompt_input = f\"Create two neutral and contextually appropriate sentences using the word '{neg_word}'. Then, provide the same sentences but with the word '{pos_word}', keeping minimal changes. enumerate the sentences generated example from 1 to 4\"\n",
    "\n",
    "# Generate LLM response\n",
    "output = replicate.run('a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5', # LLM model\n",
    "                        input={\"prompt\": f\"{pre_prompt} {prompt_input} Assistant: \", # Prompts\n",
    "                        \"temperature\":0.1, \"top_p\":0.9, \"max_length\":50, \"repetition_penalty\":1})  # Model parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Of course! I'm here to help. I will do my best to provide helpful and accurate responses while ensuring a safe and respectful interaction. Please go ahead and ask your questions.\n",
      "\n",
      "Here are two neutral and contextually appropriate sentences using the word 'irresponsible':\n",
      "\n",
      "1. The company's decision to ignore environmental regulations has been irresponsible and harmful to the planet.\n",
      "2. The driver's irresponsible behavior on the road put the lives of other motorists at risk.\n",
      "\n",
      "And here are the same sentences with the word 'responsible', keeping minimal changes:\n",
      "\n",
      "1. The company's decision to prioritize environmental sustainability has been responsible and beneficial for the planet.\n",
      "2. The driver's responsible behavior on the road ensured the safety of other motorists.\n",
      "\n",
      "I hope these sentences help illustrate the difference between 'irresponsible' and 'responsible' in a neutral and contextually appropriate way. If you have any further questions, please don't hesitate to ask.\n"
     ]
    }
   ],
   "source": [
    "full_response = \"\"\n",
    "\n",
    "for item in output:\n",
    "  full_response += item\n",
    "\n",
    "print(full_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ReplicateError",
     "evalue": "ReplicateError Details:\ntitle: Unauthenticated\nstatus: 401\ndetail: You did not pass an authentication token",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mReplicateError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 49\u001b[0m\n\u001b[0;32m     44\u001b[0m pos_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrational\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     45\u001b[0m model_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.2\u001b[39m,  \u001b[38;5;66;03m# Example of overriding a default parameter\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m     48\u001b[0m }\n\u001b[1;32m---> 49\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneg_word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_word\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 32\u001b[0m, in \u001b[0;36mgenerate_sentences\u001b[1;34m(neg_word, pos_word, model_params)\u001b[0m\n\u001b[0;32m     29\u001b[0m     default_params\u001b[38;5;241m.\u001b[39mupdate(model_params)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Generate LLM response\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mreplicate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# LLM model\u001b[39;49;00m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpre_prompt\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprompt_input\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m Assistant: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Prompts\u001b[39;49;00m\n\u001b[0;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdefault_params\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Unpack the parameters dictionary\u001b[39;49;00m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\replicate\\client.py:165\u001b[0m, in \u001b[0;36mClient.run\u001b[1;34m(self, ref, input, **params)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    157\u001b[0m     ref: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28minput\u001b[39m: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams: Unpack[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions.CreatePredictionParams\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    160\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Any, Iterator[Any]]:  \u001b[38;5;66;03m# noqa: ANN401\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m    Run a model and wait for its output.\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m run(\u001b[38;5;28mself\u001b[39m, ref, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\replicate\\run.py:40\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(client, ref, input, **params)\u001b[0m\n\u001b[0;32m     37\u001b[0m version, owner, name, version_id \u001b[38;5;241m=\u001b[39m identifier\u001b[38;5;241m.\u001b[39m_resolve(ref)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 40\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mpredictions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     41\u001b[0m         version\u001b[38;5;241m=\u001b[39mversion_id, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m {}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m     42\u001b[0m     )\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m owner \u001b[38;5;129;01mand\u001b[39;00m name:\n\u001b[0;32m     44\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mpredictions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     45\u001b[0m         model\u001b[38;5;241m=\u001b[39m(owner, name), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m {}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m     46\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\replicate\\prediction.py:462\u001b[0m, in \u001b[0;36mPredictions.create\u001b[1;34m(self, model, version, deployment, input, *args, **params)\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Deployments(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client)\u001b[38;5;241m.\u001b[39mpredictions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m    451\u001b[0m         deployment\u001b[38;5;241m=\u001b[39mdeployment,\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    454\u001b[0m     )\n\u001b[0;32m    456\u001b[0m body \u001b[38;5;241m=\u001b[39m _create_prediction_body(\n\u001b[0;32m    457\u001b[0m     version,\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    460\u001b[0m )\n\u001b[1;32m--> 462\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/predictions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _json_to_prediction(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client, resp\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\replicate\\client.py:88\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, method, path, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m httpx\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m     87\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mrequest(method, path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 88\u001b[0m     \u001b[43m_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\daria\\anaconda3\\envs\\negation\\lib\\site-packages\\replicate\\client.py:375\u001b[0m, in \u001b[0;36m_raise_for_status\u001b[1;34m(resp)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_for_status\u001b[39m(resp: httpx\u001b[38;5;241m.\u001b[39mResponse) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m400\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m600\u001b[39m:\n\u001b[1;32m--> 375\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ReplicateError\u001b[38;5;241m.\u001b[39mfrom_response(resp)\n",
      "\u001b[1;31mReplicateError\u001b[0m: ReplicateError Details:\ntitle: Unauthenticated\nstatus: 401\ndetail: You did not pass an authentication token"
     ]
    }
   ],
   "source": [
    "import replicate\n",
    "\n",
    "def generate_sentences(neg_word, pos_word, model_params=None):\n",
    "    \"\"\"\n",
    "    Generate sentences with given negative and positive words using a language model.\n",
    "\n",
    "    Args:\n",
    "        neg_word (str): The negative word to use in the sentences.\n",
    "        pos_word (str): The positive word to use in the sentences.\n",
    "        model_params (dict, optional): A dictionary of parameters for the model. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        str: The response from the language model.\n",
    "    \"\"\"\n",
    "    # Define the pre-prompt and prompt input with placeholders for the words\n",
    "    pre_prompt = \"You are a helpful assistant. You generate neutral, informative, and contextually appropriate sentences.\"\n",
    "    prompt_input = f\"Create two neutral and contextually appropriate sentences using the word '{neg_word}'. Then, provide the same sentences but with the word '{pos_word}', keeping minimal changes.\"\n",
    "\n",
    "    # Default model parameters if not provided\n",
    "    default_params = {\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_length\": 500,\n",
    "        \"repetition_penalty\": 1\n",
    "    }\n",
    "\n",
    "    # Update default parameters with any provided parameters\n",
    "    if model_params is not None:\n",
    "        default_params.update(model_params)\n",
    "\n",
    "    # Generate LLM response\n",
    "    output = replicate.run(\n",
    "        'a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5',  # LLM model\n",
    "        input={\n",
    "            \"prompt\": f\"{pre_prompt} {prompt_input} Assistant: \",  # Prompts\n",
    "            **default_params  # Unpack the parameters dictionary\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "neg_word = 'irrational'\n",
    "pos_word = 'rational'\n",
    "model_params = {\n",
    "    \"temperature\": 0.2,  # Example of overriding a default parameter\n",
    "    \"max_length\": 500\n",
    "}\n",
    "output = generate_sentences(neg_word, pos_word,model_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "negation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
